{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seismosmsr/machine_learning/blob/main/revised_conditional_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y50GdAhdqtgY"
      },
      "source": [
        "# Conditional GAN\n",
        "**Author:** [Aron Boettcher](spectral.online)<br>\n",
        "**Date created:** 2022/07/13<br>\n",
        "**Last modified:** 2022/08/12<br>\n",
        "**Description:** Training a GAN conditioned on landsat pixels to generate GEDI waveforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-meO-8ffqtgZ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Generative Adversarial Networks (GANs) let us generate novel image data, video data,\n",
        "or audio data from a random input. Typically, the random input is sampled\n",
        "from a normal distribution, before going through a series of transformations that turn\n",
        "it into something plausible (image, video, audio, etc.). This principal has been extended to sequence data in the form of stock prices, language models, and music. \n",
        "\n",
        "However, a simple [DCGAN](https://arxiv.org/abs/1511.06434) doesn't let us control\n",
        "the appearance (e.g. class) of the samples we're generating. For instance,\n",
        "with a GAN that generates MNIST handwritten digits, a simple DCGAN wouldn't let us\n",
        "choose the class of digits we're generating.\n",
        "To be able to control what we generate, we need to _condition_ the GAN output\n",
        "on a semantic input, such as the class of an image. This principal of conditioning can be extended to non-categorical inputs as well. Any condition able to be represented as a vector can be used to create a set of conditions underwhich an expected representation might be considered reasonable.\n",
        "\n",
        "In this example, we'll build a **Conditional GAN** that can generate full waveform lidar profiles conditioned on a time series of multispectral pixel values. We'll be using a time series vector of multi spectral output from the [Landsat 8](https://www.usgs.gov/landsat-missions/landsat-8) sensor to condition the generation of a full waveform lidar profile from the [GEDI sensor](https://gedi.umd.edu/instrument/instrument-overview/). Landsat 8 and the landsat time series of multispectral earth observations represent a continuous time series of observations from 1984 until the present time. While these observations were made with various mission instruments, harmonized time series have been made available and are openly accessable on [Google Earth Engine](https://earthengine.google.com/). For this exercise, we'll be using the [USGS Landsat 8 Level 2, Collection 2, Tier 1](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C02_T1_L2) product. This is the atmospherically corrected surface reflectance and land surface temperature product, from which we'll be using the coastal blue, blue, green, red, near infrared, shortwave infrared bands 1 and 2, and the kelving band. This represents 8 spectral measurements at the time of sensing.  At the time of the compostion of this notebook, this is the prefered product, owing to the Landsat 8 sensor being the most up-to-date and current sensor with a time series product available at a reasonable concurrency with the more recent GEDI data. Since we'll be sampling a range of GEDI waveforms taken over a period of approximately 3 years, and we want to use a deep time series of spectral data for conditioning, its imporant that we have full and complete coverage over this time period. The GEDI data we'll be using for this exercise were accessed using this [utitlity developed by the Earthshot Forest team](https://github.com/earthshot-forest/forest-team-gedi-etl). GEDI is a full waveform spaceborne photon counting sensor mounted to the instrument arm of the ISS, and has been collecting measurements in a 600m swath since 2017. Level 2A data were accessed in January of 2022, and their full waveform profiles used to access all landsat 8 measurements from 400 days previous to and up to 400 days after a GEDI waveform was generated. The following GEDI script can be used to perform [this utility in Earth Engine](https://code.earthengine.google.com/?scriptPath=users%2Fseismosmsr%2FForest_Structure_GAN%3AExtract_Dixie_Points).\n",
        "\n",
        "\n",
        "A model that can generate full waveform predictions from spectral data can serve various useful applications and overcome many of the limitations of the existing infrastructure for earth forest observation currently available. While the GEDI sensor is still operational, its original mission was scheduled a 3 year mission. Owing to its design constraints and the high priority for realestate on the instrument wing of the ISS, this sensor will eventually be decommissioned and as of yet, there is no plan to launch a stand alone sensor. Likewise, the GEDI mission is a relatively young mission. We currently have around 4 years of GEDI coverage across the globe. These data were taken under varying atmospheric and forest cover conditions; having a tool that allows us to align those conditions to a single capture date can be very powerful. GEDI also represents a global training dataset between 57.5 degrees latitude north and south. This is the most extensive and complete dataset taken with the same instrument and gives us the opprutnity to train regionally appropriate models in an appropriate framework. Having such a framework opens up addtional avenues for exploration such as downsampling of landsat pixels and predicting canopy structure, or extending these data to another higher resolution sensor suite like the Copernicus mission or the Planet Labs planetscope multi spectral product.\n",
        "\n",
        "## Sources of data and processing steps\n",
        "\n",
        "\n",
        "### Data access\n",
        "Two sources of data were used in this project. Firstly, GEDI fullwaveform, level 2A data were accesssed in January 2022 over the footprint of the Dixie Fire in California.\n",
        "\n",
        "[Insert map object here]\n",
        "\n",
        "\n",
        "### Data augmentation\n",
        "This region was selected as a part of a broader study into the relationship between wildfire severity and canopy structure. Landsat 8 Level 2, Collection 2, Tier 1 pixels at the foot print of each fullwave form profile were accessed for up to 400 days before and after the date where a full waveform profile was generated. The coastal blue, blue, green, red, near infrared, shortwave infrared bands 1 and 2, and the kelving band were collated along a time series vector (64 timestamps) to create a 512 long times series vector of spectral emasurements.\n",
        "[Table of spectral values]\n",
        "\n",
        "This time series of data aligned to each waveform footprint, and segmented (plus minus) to 64 flyovers (~1024 days,or 2.75 years). This process resulted in several hundred possible spectral sequences for each waveform. The distance in time from wave form origination was recored and annoted for each full waveform, spectral profile pair.\n",
        "\n",
        "[Drawing of relationship between waveform and spectral sequnces]\n",
        "\n",
        "### Data splits\n",
        "Spectral and waveform vectors were post-processed in R, resulting in ~20 million spectral sequences for several hundred thousand unique waveforms. Sequences were divided into training and test pools based on waveform ID, to prevent the potential of autocorrelation in a generative framework by have identical waveforms in both training and test dataset. Owing to the large availablity of data, a 99:1 split of training to test data was made based on waveform ID. \n",
        "\n",
        "### Data management\n",
        "These data were loaded into cloud stoarge and are downloaded as apart of running this notebook.\n",
        "\n",
        "### Other data considerations\n",
        "The extensiveness of reference data available for this exercise offers several oppurtunities not explored in this work. Here, we use the full dataset of spectral/ waveform combinations with no filtering based on cloud cover or waveform quality, or distance in time from waveform measurement. Likewise, no smoothing or interpolation by band was implemented. The goal of this exercise is to identify how well the system can overcome and predict waveforms in what effectively represents the 'worst' of potential conditions. Several data augmentations could be made at the processing phase which might improve these results. Firstly and most readily, data could be filtered down to the most recent spectral dataset associated with the collection date of a waveform. Another option would be to allign spectral sequences to a common annual date (spectral sequences always begin in spring or so). Pixels could be filtered and ommited by Percent clouds. Finally, data could be interpolated and smoothed using something like a loess smoothing function. While all of these are interesting options that are likely to improve the perfomrance of the model, the current exercise is to examine the utility of doing effectively nothing to improve the quality of the data. In a sense, we're trying to examine the utility of this framework under the worst possible conditions and so these additional data augmentation steps are not explored here.\n",
        "\n",
        "## Modeling framework\n",
        "\n",
        "Generative models are notorious for the dificulty in developing and modeling their training. In an effort to determine the effectiveness of this approach, a multi stage approach was used for model development to ensure component models at least had the potential to generate reasonable results. Prior to implementing a conditional generative framework, an recurrent neural networks (RNN)   implementation for the prediction of waveform from spectral data was developed. The goal behind this stage was to ensure that meaningfully decent predicitions could of waveform could be made from spectral data. Second, a generative approach  with no conditioning was used to ensure that waveforms could effectively be generated and discriminated. Finally, these approaches are unified in a conditional generative approach were spectral conditional data are married with either generated waveform or reference waveform data in a conditional generative framwork.\n",
        "\n",
        "### RNN prediction of full waveforms from spectral time series data\n",
        "\n",
        "Recurrent neural networks (RNN) have demonstrated extraordiniry utility in the prediction of sequnce data (cite this). As well, some of the most recent and most advanced machine learning frameworks are based on an RNN configuraton (cite this). Long term short memory (LSTM) units are a subclass of RNN and have have found their way into the prediction of sequnce data. These frameworks have been developed and implemented to predict a wide range of sequence data like stock prices, natural language, and music. Owing to these prior works, I decided to use a multiple LSTM framework to vet out how effectively this framework could predict waveform from time series spectral data.\n",
        "\n",
        "I implemented a 3 unit LSTM with each subunit composed of dense layer, followed by a dropout layer, followed by an LSTM layer. Three of these subunits in a row compose the full RNN. Each epoch a sample of 500,000 samples of training data were taken, along with 2000 samples of test data; training and validation loss and accuracy were recored printed every epoch and once validation accuracy appeared to have stabilized, training was halted. A final sample of 20,000 samples of test data were used to construct validation statistics.\n",
        "\n",
        "Part of the goal of this process was to ensure that at least individuall, the component parts of the conditional GAN would be effective. I explored a range of other network structures including fully connected models, convolutional models, and very deep networks. Withthese networks I did not implement the network or compute and test statistics, but rather, visualized the output on training data to see if it produced decent results. The central issue with these other frameworks was that nodes did not appear to have enough node-node correlation in the final layer, resulting in predictions that were jagged and did not have the between unit smoothness demonstrated by the LSTM framework. \n",
        "\n",
        "### Generative adversarial network\n",
        "\n",
        "Prior to implementing a conditional GAN, I wanted to explore the question of if it was even possible to generate waveform like data. This also represents an oppurutnity to vet out modeling framework and identify a configuration for a discriminator model that should be suitable in a condional framework. Only data from the training dataset were used to develop the GAN framework. Models were evaluated based on a visual inspection of their generated results.\n",
        "\n",
        "\n",
        "\n",
        "## Coding references\n",
        "Following are the references used for developing this example (a full suite of reference is available at the end of this document as an appendix):\n",
        "\n",
        "* [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784)\n",
        "\n",
        "* [Lecture on Conditional Generation from Coursera](https://www.coursera.org/lecture/build-basic-generative-adversarial-networks-gans/conditional-generation-inputs-2OPrG)\n",
        "\n",
        "If you need a refresher on GANs, you can refer to the \"Generative adversarial networks\"\n",
        "section of\n",
        "[this resource](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-12/r-3/232).\n",
        "\n",
        "\n",
        "This example requires TensorFlow 2.5 or higher, as well as TensorFlow Docs, which can be\n",
        "installed using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V8Zi-apyqtgZ",
        "outputId": "461501cd-a407-486a-adee-57c44bad753a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4an01rqtga"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M4msdB2Qqtga"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow_docs.vis import embed\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lTXrmg8qtga"
      },
      "source": [
        "## Constants and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7RCu_-uVqtga"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "num_channels = 1\n",
        "num_classes = 101\n",
        "image_size = 512\n",
        "latent_dim = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw35q14Sqtgb"
      },
      "source": [
        "## Loading the dataset and sampling function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Get training data and test data from google drive\n",
        "url = 'https://drive.google.com/uc?id=1kwU_CkP1X5M1CDlYKPkb9p5J8GulWBXr'\n",
        "output = 'sample_test.csv'\n",
        "gdown.download(url,output,quiet = False)\n",
        "\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1graojPHaMvZGHmocsUFLds9bFWQG_mr6'\n",
        "output = 'sample_train.csv'\n",
        "gdown.download(url,output,quiet = False)"
      ],
      "metadata": {
        "id": "9724Jbw9sRSb",
        "outputId": "bc56096e-448c-4545-ce5f-b0c0b33ea136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kwU_CkP1X5M1CDlYKPkb9p5J8GulWBXr\n",
            "To: /content/sample_test.csv\n",
            "100%|██████████| 137M/137M [00:01<00:00, 108MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1graojPHaMvZGHmocsUFLds9bFWQG_mr6\n",
            "To: /content/sample_train.csv\n",
            "100%|██████████| 13.8G/13.8G [01:28<00:00, 155MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sample_train.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function for sampling training and test data"
      ],
      "metadata": {
        "id": "TyZSS9h0yzI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate n real samples with class labels\n",
        "def generate_real_samples(s,filename = '/content/sample.csv',shuffle=True):\n",
        "  import random\n",
        "  import pandas\n",
        "  import numpy\n",
        "\t# Sample s rows of data.frame\n",
        "\n",
        "  #number of records in file (excludes header)\n",
        "  n = sum(1 for line in open(filename)) - 1 \n",
        "\n",
        "  #the 0-indexed header will not be included in the skip list\n",
        "  skip = sorted(random.sample(range(1,n+1),n-s)) \n",
        "  df = pandas.read_csv(filename, skiprows=skip)\n",
        "  X = []\n",
        "  for i in range(len(df.index)):\n",
        "    text_exa = df['rh'][i]\n",
        "    text_exa = str(text_exa).replace(\"{\",\"\").replace(\"}\", \"\")\n",
        "    test_exa = text_exa.split(\",\")\n",
        "    test_exa = [float(i) for i in test_exa]\n",
        "    # test_exa = test_exa\n",
        "    X.append(test_exa)\n",
        "\n",
        "\n",
        "\t# generate class labels\n",
        "  y = []\n",
        "  for i in range(len(df.index)):\n",
        "    # y_one = numpy.ones(1)\n",
        "    y_one = df['ls'][i]\n",
        "    y_one = str(y_one).replace(\"{\",\"\").replace(\"}\", \"\")\n",
        "    y_one = y_one.split(\",\")\n",
        "    y_one = [float(i) for i in y_one]\n",
        "    y.append(y_one)\n",
        "\n",
        "\t# waveform id labels\n",
        "  z = []\n",
        "  for i in range(len(df.index)):\n",
        "    z_one = df['shot_number'][i]\n",
        "    z.append(z_one)\n",
        "    \n",
        "  X = numpy.array(X)\n",
        "  y = numpy.array(y)\n",
        "\n",
        "  #Apply a normalization\n",
        "  X = (((X.astype(\"float32\")+100)/255)).astype(\"float32\")\n",
        "  y =(y.astype(\"float32\")/ 65455.0).astype(\"float32\")\n",
        "\n",
        "#  np.array([33,11,22], dtype=int)\n",
        "  if shuffle == True:\n",
        "    i = [i for i in range(len(X))]\n",
        "    random.shuffle(i)\n",
        "    X = X[i]\n",
        "    y = y[i]\n",
        "    z = np.array(z)[i]\n",
        "  return X, y, z"
      ],
      "metadata": {
        "id": "Bu4Q-00Hu63I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35LbRuf5qtgb"
      },
      "source": [
        "## Predicting waveform from spectral time series.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Input\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Concatenate\n",
        "from keras import Model\n",
        "\n",
        "input_layer = Input(shape=(512))\n",
        "\n",
        "dense1 = Dense(1024, activation='relu')(input_layer)\n",
        "resh1 = Reshape(target_shape=(2,512))(dense1)\n",
        "dropout1 =  Dropout(.25)(resh1)\n",
        "lstm1 = LSTM(32, activation=\"relu\",return_sequences=True)(dropout1)\n",
        "\n",
        "\n",
        "dense2 = Dense(512, activation='relu')(lstm1)\n",
        "resh2 = Reshape(target_shape=(2,512))(dense2)\n",
        "dropout2 =  Dropout(.25)(resh2)\n",
        "lstm2 = LSTM(32, activation=\"relu\",return_sequences=True)(dropout2)\n",
        "\n",
        "\n",
        "dense3 = Dense(512, activation='relu')(lstm2)\n",
        "resh3 = Reshape(target_shape=(2,512))(dense3)\n",
        "dropout3 =  Dropout(.25)(resh3)\n",
        "lstm3 = LSTM(32, activation=\"relu\",return_sequences=True)(dropout3)\n",
        "\n",
        "dense4 = Dense(512, activation='relu')(lstm3)\n",
        "flat6 = Flatten()(dense4)\n",
        "\n",
        "dense4 = Dense(1024, activation='relu')(flat6)\n",
        "\n",
        "output_layer = Dense(101, activation='linear')(dense4)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "optz = keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(loss=\"MeanSquaredError\", optimizer=optz,metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "57KkLbzGmeih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e985f6f-3c5c-4c55-f68b-00d1c2b56791"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  (x_train, y_train,z_train) = generate_real_samples(200000,filename = '/content/sample_train.csv')\n",
        "  (x_test, y_test, z_train) = generate_real_samples(2000,filename = '/content/sample_test.csv')\n",
        "\n",
        "\n",
        "  dataset_train = tf.data.Dataset.from_tensor_slices((y_train,x_train))\n",
        "  dataset_train = dataset_train.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "  dataset_test = tf.data.Dataset.from_tensor_slices((y_test,x_test))\n",
        "  dataset_test = dataset_test.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "  model.fit(dataset_train,validation_data =dataset_test , epochs=20)\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  import math\n",
        "  # (real_labels, real_pixels, waveform_labels) = generate_real_samples(20000,filename = '/content/sample_test.csv')\n",
        "  p_test = model.predict(y_test)\n",
        "\n",
        "\n",
        "  err_vec = ((p_test*255 - 100)-(x_test*255 - 100))\n",
        "  rse_vec = np.sqrt(err_vec**2)\n",
        "  std_vec = np.std(rse_vec,axis = 0)\n",
        "  mean_err_vec = np.mean(rse_vec,axis=0)\n",
        "\n",
        "  mean_label_vec = (np.mean(x_test,axis=0))\n",
        "  mean_pred_vec = (np.mean(p_test,axis=0))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  for i in range(100):\n",
        "    ax.plot(p_test[i]*255 - 100,x_test[i]*255 - 100,label=i)\n",
        "  ax.plot(mean_pred_vec*255 - 100,mean_label_vec*255 - 100,color='black')\n",
        "  ax.set(xlabel='predicted waveform', ylabel='truth waveform',\n",
        "        title='About as simple as it gets, folks')\n",
        "  ax.grid()\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # ax.plot(mean_label_vec,mean_err_vec,color='black')\n",
        "  ax.plot(range(101),mean_err_vec,color='black')\n",
        "  ax.plot(range(101),mean_err_vec+std_vec*2,color='red')\n",
        "  ax.plot(range(101),mean_err_vec+std_vec,color='orange')\n",
        "  ax.set(xlabel='percentile', ylabel='rms error',\n",
        "        title='About as simple as it gets, folks')\n",
        "  ax.grid()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "daL_IqxrawiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "(real_labels, real_pixels, waveform_labels) = generate_real_samples(20000,filename = '/content/sample_test.csv')\n",
        "p_test = model.predict(real_pixels)\n",
        "\n",
        "\n",
        "err_vec = ((p_test*255 - 100)-(real_labels*255 - 100))\n",
        "rse_vec = np.sqrt(err_vec**2)\n",
        "std_vec = np.std(rse_vec,axis = 0)\n",
        "mean_err_vec = np.mean(rse_vec,axis=0)\n",
        "\n",
        "mean_label_vec = (np.mean(real_labels,axis=0))\n",
        "mean_pred_vec = (np.mean(p_test,axis=0))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for i in range(100):\n",
        "  ax.plot(p_test[i]*255 - 100,real_labels[i]*255 - 100,label=i)\n",
        "ax.plot(mean_pred_vec*255 - 100,mean_label_vec*255 - 100,color='black')\n",
        "ax.set(xlabel='predicted waveform', ylabel='truth waveform',\n",
        "       title='About as simple as it gets, folks')\n",
        "ax.grid()\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# ax.plot(mean_label_vec,mean_err_vec,color='black')\n",
        "ax.plot(range(101),mean_err_vec,color='black')\n",
        "ax.plot(range(101),mean_err_vec+std_vec*2,color='red')\n",
        "ax.plot(range(101),mean_err_vec+std_vec,color='orange')\n",
        "ax.set(xlabel='percentile', ylabel='rms error',\n",
        "       title='About as simple as it gets, folks')\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-7MICnxHhgk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import LeakyReLU, Activation, Input, Dense, Dropout, Concatenate, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l1_l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "j6AXF9CVl-Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gan(generator, discriminator, name=\"gan\"):\n",
        "    '''Build the GAN from a generator and a discriminator'''\n",
        "    yfake = Activation(\"linear\", name=\"yfake\")(discriminator(generator(generator.inputs)))\n",
        "    yreal = Activation(\"linear\", name=\"yreal\")(discriminator(discriminator.inputs))\n",
        "    model = Model(generator.inputs + discriminator.inputs, [yfake, yreal], name=name)\n",
        "    return model\n",
        "    "
      ],
      "metadata": {
        "id": "wPyXK81VmBVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        # layers.Dense(1024, activation='relu'),\n",
        "        # layers.Reshape(target_shape=(2,512)),\n",
        "        # layers.Dropout(.25),\n",
        "        # layers.LSTM(32, activation=\"relu\",return_sequences=True),\n",
        "     \n",
        "        # layers.Dense(512, activation='relu'),\n",
        "        # layers.Reshape(target_shape=(2,512)),\n",
        "        # layers.Dropout(.25),\n",
        "        # layers.LSTM(32, activation=\"relu\",return_sequences=True),\n",
        "     \n",
        "        # layers.Dense(512, activation='relu'),\n",
        "        # layers.Reshape(target_shape=(2,512)),\n",
        "        # layers.Dropout(.25),\n",
        "        # layers.LSTM(32, activation=\"relu\",return_sequences=True),\n",
        "     \n",
        "        # layers.Flatten(),\n",
        "        # layers.Dense(512, activation='relu'),"
      ],
      "metadata": {
        "id": "Glk9aBCdyb90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original Discriminator\n",
        "def disc(image_dim, label_dim, layer_dim=1024, reg=lambda: l1_l2(1e-5, 1e-5)):\n",
        "    '''Discriminator network'''\n",
        "    x      = (Input(shape=(image_dim,), name='discriminator_input'))\n",
        "    label  = (Input(shape=(label_dim,), name='discriminator_label'))\n",
        "    inputs = (Concatenate(name='input_concatenation'))([x, label])\n",
        "\n",
        "    a = (Dense(layer_dim, name=\"discriminator_h1\", kernel_regularizer=reg()))(inputs)\n",
        "    a = (LeakyReLU(0.2))(a)\n",
        "    a = (Dense(int(layer_dim / 2), name=\"discriminator_h2\", kernel_regularizer=reg()))(a)\n",
        "    a = (LeakyReLU(0.2))(a)\n",
        "    a = (Dense(int(layer_dim / 4), name=\"discriminator_h3\", kernel_regularizer=reg()))(a)\n",
        "    a = (LeakyReLU(0.2))(a)\n",
        "    a = (Dense(1, name=\"discriminator_y\", kernel_regularizer=reg()))(a)\n",
        "    a = (Activation('sigmoid'))(a)\n",
        "    model = Model(inputs=[x, label], outputs=a, name=\"discriminator\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "f96gDr-KmCs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def disc(image_dim, label_dim, layer_dim=1024, reg=lambda: l1_l2(1e-5, 1e-5)):\n",
        "#     '''Discriminator network'''\n",
        "#     x      = (Input(shape=(image_dim,), name='discriminator_input'))\n",
        "#     label  = (Input(shape=(label_dim,), name='discriminator_label'))\n",
        "#     inputs = (Concatenate(name='input_concatenation'))([x, label])\n",
        "    \n",
        "#     a = (Dense(int(layer_dim), name=\"generator_h1\", kernel_regularizer=reg()))(inputs)\n",
        "#     a = (LeakyReLU(0.2))(a)    # Trick 5\n",
        "#     a = (Dense(layer_dim, name=\"discriminator_h1\", kernel_regularizer=reg()))(inputs)\n",
        "#     a = (LeakyReLU(0.2))(a)\n",
        "#     a = (Dense(int(layer_dim ), name=\"discriminator_h2\", kernel_regularizer=reg()))(a)\n",
        "#     a = (LeakyReLU(0.2))(a)\n",
        "#     a = (Dense(int(layer_dim ), name=\"discriminator_h3\", kernel_regularizer=reg()))(a)\n",
        "#     a = (LeakyReLU(0.2))(a)\n",
        "#     # a = (Dense(1, name=\"discriminator_y\", kernel_regularizer=reg()))(a)\n",
        "#     a = (Reshape(target_shape=(2,int(layer_dim/2))))(a)\n",
        "#     a = (Dropout(0.25))(a)\n",
        "#     a = (LSTM(32, activation=\"relu\",return_sequences=True)(a))\n",
        "#     a = (Dense(int(layer_dim/2),activation = \"relu\"))(a)\n",
        "#     a = (Dropout(0.25))(a)\n",
        "#     a = (LSTM(32, activation=\"relu\",return_sequences=True)(a))\n",
        "#     a = (Dense(int(layer_dim/2),activation = \"relu\"))(a)\n",
        "#     a = (Dropout(0.25))(a)\n",
        "#     a = (LSTM(32, activation=\"relu\",return_sequences=True)(a))\n",
        "#     a = (Dense(int(layer_dim/2),activation = \"relu\"))(a)\n",
        "#     a = (Flatten())(a)\n",
        "\n",
        "#     a = (Dense(1, name=\"discriminator_y\", kernel_regularizer=reg()))(a)\n",
        "#     a = (Activation('sigmoid'))(a)\n",
        "#     model = Model(inputs=[x, label], outputs=a, name=\"discriminator\")\n",
        "#     return model"
      ],
      "metadata": {
        "id": "YUtT3TOL0n8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Original generator\n",
        "def gen(noise_dim, label_dim, image_dim, layer_dim=4096, activ='tanh', reg=lambda: l1_l2(1e-5, 1e-5)):\n",
        "    '''Generator network'''\n",
        "    z      = (Input(shape=(noise_dim,), name='generator_input'))\n",
        "    label  = (Input(shape=(label_dim,), name='generator_label'))\n",
        "    inputs = (Concatenate(name='input_concatenation'))([z, label])\n",
        "    a = (Dense(int(layer_dim / 4), name=\"generator_h1\", kernel_regularizer=reg()))(inputs)\n",
        "    a = (LeakyReLU(0.2))(a)    # Trick 5\n",
        "    a = (Dense(int(layer_dim / 2), name=\"generator_h2\", kernel_regularizer=reg()))(a)\n",
        "    a = (LeakyReLU(0.2))(a)\n",
        "    a = (Dense(layer_dim, name=\"generator_h3\", kernel_regularizer=reg()))(a)\n",
        "    a = (LeakyReLU(0.2))(a)\n",
        "    a = (Dense(np.prod(image_dim), name=\"generator_x_flat\", kernel_regularizer=reg()))(a)\n",
        "    a = (Activation(activ))(a)    \n",
        "    model = Model(inputs=[z, label], outputs=[a, label], name=\"generator\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "JQ71OQrEiFiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Generator revision one\n",
        "# def gen(noise_dim, label_dim, image_dim, layer_dim=1024, activ='tanh', reg=lambda: l1_l2(1e-5, 1e-5)):\n",
        "#     '''Generator network'''\n",
        "#     z      = (Input(shape=(noise_dim,), name='generator_input'))\n",
        "#     label  = (Input(shape=(label_dim,), name='generator_label'))\n",
        "#     inputs = (Concatenate(name='input_concatenation'))([z, label])\n",
        "\n",
        "#     a = (Dense(int(layer_dim), name=\"generator_h1\", kernel_regularizer=reg()))(inputs)\n",
        "#     a = (LeakyReLU(0.2))(a)    # Trick 5\n",
        "#     a = (Dense(int(layer_dim ), name=\"generator_h2\", kernel_regularizer=reg()))(a)\n",
        "#     a = (LeakyReLU(0.2))(a)    # Trick 5\n",
        "#     a = (Dense(int(layer_dim), name=\"generator_h3\", kernel_regularizer=reg()))(a)\n",
        "#     a = (LeakyReLU(0.2))(a)\n",
        "#     a = (Dense(layer_dim, name=\"generator_h4\", kernel_regularizer=reg()))(a)\n",
        "#     a = (LeakyReLU(0.2))(a)\n",
        "#     a = (Reshape(target_shape=(2,int(layer_dim/2))))(a)\n",
        "#     a = (Dropout(0.25))(a)\n",
        "#     a = (LSTM(32, activation=\"relu\",return_sequences=True)(a))\n",
        "#     a = (Dense(int(layer_dim/2),activation = \"relu\"))(a)\n",
        "#     a = (Dropout(0.25))(a)\n",
        "#     a = (LSTM(32, activation=\"relu\",return_sequences=True)(a))\n",
        "#     a = (Dense(int(layer_dim/2),activation = \"relu\"))(a)\n",
        "#     a = (Dropout(0.25))(a)\n",
        "#     a = (LSTM(32, activation=\"relu\",return_sequences=True)(a))\n",
        "#     a = (Dense(int(layer_dim/2),activation = \"relu\"))(a)\n",
        "#     a = (Flatten())(a)\n",
        "\n",
        "\n",
        "#     a = (Dense(np.prod(image_dim), name=\"generator_x_flat\", kernel_regularizer=reg()))(a)\n",
        "#     a = (Activation(activ))(a)    \n",
        "#     model = Model(inputs=[z, label], outputs=[a, label], name=\"generator\")\n",
        "#     return model"
      ],
      "metadata": {
        "id": "k_mbMm_nXr5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_trainable(net, val):\n",
        "    '''Changes the trainable property of a model as a whole and layer by layer'''\n",
        "    net.trainable = val\n",
        "    for l in net.layers:\n",
        "        l.trainable = val"
      ],
      "metadata": {
        "id": "Y7JUlzRqmH41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Data preparation\n",
        "# ------------------------------------------------------------------------------\n",
        "# (x_train, l_train), (x_test, l_test) = mnist.load_data()\n",
        "# x_train = np.concatenate((x_train, x_test))\n",
        "# l_train = np.concatenate((l_train, l_test))\n",
        "\n",
        "# # Normalization according to Trick 1\n",
        "# x_train = x_train.reshape(x_train.shape[0], 784)\n",
        "# x_train = (x_train - 127.5) / 127.5\n",
        "# l_train = to_categorical(l_train)"
      ],
      "metadata": {
        "id": "x-7jCP8MmR_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (real_labels_sample, real_pixels_sample, real_waveform_labels) = generate_real_samples(200000,filename = '/content/sample_train.csv')"
      ],
      "metadata": {
        "id": "2PxhfBz67OCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# real_pixels_sample[0]"
      ],
      "metadata": {
        "id": "zKBJuKmIBF8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train = real_pixels_sample\n",
        "# l_train = real_labels_sample\n",
        "\n",
        "l_train = real_pixels_sample\n",
        "x_train = real_labels_sample"
      ],
      "metadata": {
        "id": "SPFUWOaT7d1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train[0]"
      ],
      "metadata": {
        "id": "wFnxxl7SH1Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Parameter choice\n",
        "# ------------------------------------------------------------------------------    \n",
        "# Dimension of noise to be fed to the generator\n",
        "noise_dim = 64\n",
        "# Dimension of images generated\n",
        "image_dim = 101\n",
        "# Dimension of labels\n",
        "label_dim = 512\n",
        "\n",
        "batch_size  = 20000\n",
        "num_batches = int(x_train.shape[0] / batch_size)\n",
        "num_epochs  = 100"
      ],
      "metadata": {
        "id": "OaXDg4DEmX7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Network creation\n",
        "# ------------------------------------------------------------------------------\n",
        "# Create generator ((z, l) -> (x, l))\n",
        "generator = gen(noise_dim, label_dim, image_dim)\n",
        "adam = Adam(lr=0.0002, beta_1=0.5)\n",
        "generator.compile(loss='linear', optimizer=adam)    # Trick 9\n",
        "\n",
        "# Create discriminator ((x, l) -> y)\n",
        "discriminator = disc(image_dim, label_dim)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='SGD')    # Trick 9\n",
        "\n",
        "# Build GAN. Note how the discriminator is set to be not trainable since the beginning\n",
        "make_trainable(discriminator, False)\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "metadata": {
        "id": "qPPV6xEImaHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://gist.github.com/apozas/38d4640d9e6525b43db62dac846f1c19"
      ],
      "metadata": {
        "id": "m7CmjUNKMPx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Training\n",
        "# ------------------------------------------------------------------------------\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
        "    for index in range(num_batches):\n",
        "        # Train the discriminator. It looks like training works best if it is trained first on only real data, and then only\n",
        "        # on fake data, so let's do that. This is Trick 4.\n",
        "        make_trainable(discriminator, True)\n",
        "        # Train dicriminator on real data\n",
        "        batch       = np.random.randint(0, x_train.shape[0], size=batch_size)\n",
        "        image_batch = x_train[batch]\n",
        "        label_batch = l_train[batch]\n",
        "        y_real      = np.ones(batch_size) + 0.2 * np.random.uniform(-1, 1, size=batch_size)    # Label smoothing. Trick 6\n",
        "        discriminator.train_on_batch([image_batch, label_batch], y_real)\n",
        "        # Train the discriminator on fake data\n",
        "        noise_batch      = np.random.normal(0, 1, (batch_size, noise_dim))    # Trick 3\n",
        "        generated_images = generator.predict([noise_batch, label_batch])\n",
        "        y_fake           = np.zeros(batch_size) + 0.2 * np.random.uniform(0, 1, size=batch_size)    # Label smoothing\n",
        "        d_loss = discriminator.train_on_batch(generated_images, y_fake)\n",
        "        \n",
        "        # Recall that generated_images already contains the labels\n",
        "        # Train the generator. We train it through the whole model. There is a very subtle point here. We want to minimize the error\n",
        "        # of the discriminator, but on the other hand we want to have the generator maximizing the loss of the discriminator (make him\n",
        "        # not capable of distinguishing which images are real). One way to achieve this is to change the loss function of the generator\n",
        "        # by some kind of \"negative loss\", which in practice is implemented by switching the labels of the real and the fake\n",
        "        # images. Note that when training the discriminator we were doing the assignment real_image->1, fake_image->0, so now\n",
        "        # we will do real_image->0, fake_image->1. The order of the outputs is [fake, real], as given by build_gan(). This is Trick 2.\n",
        "        \n",
        "        make_trainable(discriminator, False)\n",
        "        gan_loss = gan.train_on_batch([noise_batch, label_batch, image_batch, label_batch], [y_real, y_fake])\n",
        "\n",
        "    print(\n",
        "        \"Batch {}/{}: Discriminator loss = {}, GAN loss = {}\".format(index + 1, num_batches, d_loss,\n",
        "                                                                      gan_loss))\n",
        "# Save weights. Just saving the whole GAN should work as well\n",
        "generator.save_weights('generator_cGAN.h5')\n",
        "discriminator.save_weights('discriminator_cGAN.h5')\n",
        "gan.save_weights('gan_cGAN.h5')\n"
      ],
      "metadata": {
        "id": "RnOBh6helcOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NyIFyaMZuYDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Plotting\n",
        "# ------------------------------------------------------------------------------\n",
        "# plt.figure(figsize=(20, 2))\n",
        "# # print(label_dim)\n",
        "# for i in range(len(generated_images[0])):\n",
        "#     im =generated_images[0][i].reshape(28,28)\n",
        "#     plt.subplot(1, label_dim, i+1)\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(im, cmap='Greys_r')\n",
        "# plt.show()\n",
        "import random\n",
        "batch       = [random.randint(0,len(x_train)) for i in range(100)]\n",
        "image_batch = x_train[batch]\n",
        "label_batch = l_train[batch]\n",
        "\n",
        "\n",
        "noise_batch      = np.random.normal(0, 1, (len(batch), noise_dim))    # Trick 3\n",
        "generated_images = generator.predict([noise_batch, label_batch])\n",
        "# print(generated_images)\n",
        "err_vec = ((np.array(generated_images[0])*255 - 100)-(np.array(image_batch)*255 - 100))\n",
        "rse_vec = np.sqrt(err_vec**2)\n",
        "std_vec = np.std(rse_vec,axis = 0)\n",
        "mean_err_vec = np.mean(rse_vec,axis=0)\n",
        "\n",
        "mean_label_vec = (np.mean(real_labels,axis=0))\n",
        "mean_pred_vec = (np.mean(p_test,axis=0))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# m = 21\n",
        "\n",
        "for i in range(len(generated_images[0])):\n",
        "  ax.plot(generated_images[0][i]*255-100, image_batch[i]*255-100,label=i)\n",
        "ax.plot(mean_pred_vec*255 - 100,mean_label_vec*255 - 100,color='black')\n",
        "ax.set(xlabel='generated waveform', ylabel='truth waveform',\n",
        "       title='About as simple as it gets, folks')\n",
        "ax.grid()\n",
        "\n",
        "# fig.savefig(\"test.png\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# ax.plot(mean_label_vec,mean_err_vec,color='black')\n",
        "ax.plot(range(101),mean_err_vec,color='black')\n",
        "ax.plot(range(101),mean_err_vec+std_vec*2,color='red')\n",
        "ax.plot(range(101),mean_err_vec+std_vec,color='orange')\n",
        "ax.set(xlabel='percentile', ylabel='rms error',\n",
        "       title='About as simple as it gets, folks')\n",
        "ax.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DxJUz5SAmfXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "revised_conditional_gan",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}