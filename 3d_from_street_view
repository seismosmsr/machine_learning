{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKCqXuF7+6nudYjdi4rceP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seismosmsr/machine_learning/blob/main/3d_from_street_view\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install google_streetview transformers\n",
        "# !pip install pyproj\n",
        "# !pip install open3d\n",
        "# !pip install geopy\n",
        "# !pip install --upgrade pillow\n",
        "# !pip uninstall transformers\n",
        "# !pip install transformers\n",
        "# !pip install albumentations\n",
        "# !pip install open3d jupyter\n",
        "# !pip install open3d\n",
        "!pip install open3d_web_visualizer\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K0RJRFlJmt1",
        "outputId": "facdc8bd-1745-4dc0-a667-4f8e17800330"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement open3d_web_visualizer (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for open3d_web_visualizer\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from google_streetview import api\n",
        "from transformers import pipeline, AutoFeatureExtractor, AutoModelForDepthEstimation\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "from geopy import Point\n",
        "import pyproj\n",
        "from pyproj import Proj\n",
        "import cv2\n",
        "import open3d as o3d\n",
        "import random\n",
        "import ipyvolume as ipv\n",
        "# from albumentations.augmentations.functional import convert_image_dtype, resize\n"
      ],
      "metadata": {
        "id": "8LcQTkLjI626"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your Google API key\n",
        "GOOGLE_API_KEY = \"AIzaSyCjfZ_8O2mhuDppDXbrnhxdK2sIYp48GOo\"\n",
        "\n",
        "# Define the Hugging Face model you want to use\n",
        "HF_MODEL = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
        "\n",
        "\n",
        "# Example coordinates (latitude, longitude)\n",
        "coordinates = (40.712776, -74.005974)  # New York City"
      ],
      "metadata": {
        "id": "TcAtYha4JAKq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to download an image and save it locally\n",
        "def download_image(url, filepath):\n",
        "    response = requests.get(url)\n",
        "    with open(filepath, \"wb\") as img_file:\n",
        "        img_file.write(response.content)\n",
        "\n",
        "# Function to get the Google Street View image URL\n",
        "def get_streetview_image_url(lat, lng, key, heading):\n",
        "    params = [{\n",
        "        \"size\": \"640x640\",\n",
        "        \"location\": f\"{lat},{lng}\",\n",
        "        \"heading\": heading,\n",
        "        \"pitch\": \"0\",\n",
        "        \"fov\": \"90\",\n",
        "        \"key\": key,\n",
        "    }]\n",
        "    results = api.results(params)\n",
        "    metadata = results.metadata[0]\n",
        "\n",
        "    if metadata[\"status\"] == \"OK\":\n",
        "        pano_id = metadata[\"pano_id\"]\n",
        "        url = f\"https://maps.googleapis.com/maps/api/streetview?size=640x640&pano={pano_id}&heading={heading}&key={key}\"\n",
        "        return url\n",
        "    else:\n",
        "        print(\"Error getting Street View image. Metadata:\", metadata)\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to classify an image using a Hugging Face model\n",
        "def segment_image(filepath, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    segmentor = pipeline(\"image-segmentation\", model=model, device=device.index if device.type == \"cuda\" else -1)\n",
        "    return segmentor(filepath)\n",
        "\n",
        "def depth_image(filepath, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    depth = pipeline(\"depth-estimation\", model=model, device=device.index if device.type == \"cuda\" else -1)\n",
        "    return depth(filepath)\n",
        "\n",
        "def overlay_masks_on_image(image_filepath, segmentation_results, alpha=0.5):\n",
        "    base_image = Image.open(image_filepath).convert(\"RGBA\")\n",
        "    overlay = Image.new(\"RGBA\", base_image.size, (255, 255, 255, 0))\n",
        "    \n",
        "    for result in segmentation_results:\n",
        "        mask = result[\"mask\"].convert(\"RGBA\")\n",
        "        color = np.random.randint(0, 256, 3).tolist() + [int(255 * alpha)]\n",
        "        colored_mask = Image.new(\"RGBA\", mask.size, tuple(color))\n",
        "        overlay.paste(colored_mask, mask=mask)\n",
        "    \n",
        "    combined_image = Image.alpha_composite(base_image, overlay)\n",
        "    return combined_image\n",
        "\n",
        "def create_false_color_image(segmentation_results, image_size):\n",
        "    label_colors = {}\n",
        "    \n",
        "    for result in segmentation_results:\n",
        "        label = result['label']\n",
        "        if label not in label_colors:\n",
        "            label_colors[label] = [random.randint(0, 255) for _ in range(3)]\n",
        "\n",
        "    false_color_image = np.zeros((image_size[1], image_size[0], 3), dtype=np.uint8)\n",
        "\n",
        "    for result in segmentation_results:\n",
        "        label = result['label']\n",
        "        mask = np.array(result['mask'])\n",
        "        color = np.array(label_colors[label], dtype=np.uint8)\n",
        "        false_color_image[mask > 128] = color\n",
        "\n",
        "    return Image.fromarray(false_color_image), label_colors\n",
        "\n",
        "\n",
        "# Function to save classes and their associated integer values to a JSON file\n",
        "def save_classes_to_json(class_order, filepath):\n",
        "    classes = {label: i for i, label in enumerate(class_order)}\n",
        "    with open(filepath, \"w\") as json_file:\n",
        "        json.dump(classes, json_file)\n",
        "\n",
        "\n",
        "def latlng_to_ecef(lat, lng):\n",
        "    # Define the WGS84 ellipsoid and the ECEF coordinate system\n",
        "    wgs84 = pyproj.Proj(proj='latlong', datum='WGS84', radians=True)\n",
        "    ecef = pyproj.Proj(proj='geocent', datum='WGS84', radians=True)\n",
        "\n",
        "    # Check if latitude and longitude are scalars\n",
        "    if not (np.isscalar(lat) and np.isscalar(lng)):\n",
        "        raise ValueError(\"Latitude and longitude must be scalars\")\n",
        "\n",
        "    # Convert lat, lng to ECEF coordinates\n",
        "    x, y, z = pyproj.transform(wgs84, ecef, lng, lat, 0, radians=True)\n",
        "\n",
        "    return x, y, z\n",
        "\n",
        "\n",
        "def calc_rotation_matrix(heading, pitch):\n",
        "    # Convert heading and pitch to radians\n",
        "    h_rad = np.radians(heading)\n",
        "    p_rad = np.radians(pitch)\n",
        "\n",
        "    # Calculate rotation matrix\n",
        "    cos_h = np.cos(h_rad)\n",
        "    sin_h = np.sin(h_rad)\n",
        "    cos_p = np.cos(p_rad)\n",
        "    sin_p = np.sin(p_rad)\n",
        "    Rz = np.array([[cos_h, -sin_h, 0], [sin_h, cos_h, 0], [0, 0, 1]])\n",
        "    Ry = np.array([[cos_p, 0, sin_p], [0, 1, 0], [-sin_p, 0, cos_p]])\n",
        "    R = Ry.dot(Rz)\n",
        "\n",
        "    return R\n",
        "\n",
        "def project_pixel(pixel, depth, fov, R):\n",
        "    # Convert pixel to normalized device coordinates (NDC)\n",
        "    ndc_x = (pixel[0] + 0.5) / depth.shape[1] - 0.5\n",
        "    ndc_y = (pixel[1] + 0.5) / depth.shape[0] - 0.5\n",
        "\n",
        "    # Convert NDC to camera coordinates\n",
        "    tan_fov = np.tan(np.radians(fov) / 2)\n",
        "    cam_x = ndc_x * depth.shape[1] / (2 * tan_fov)\n",
        "    cam_y = ndc_y * depth.shape[0] / (2 * tan_fov)\n",
        "    cam_z = -depth[pixel[1], pixel[0]]\n",
        "\n",
        "    # Convert camera coordinates to world coordinates\n",
        "    cam_pos = np.array([0, 0, 0])\n",
        "    world_pos = R.T.dot(np.array([cam_x, cam_y, cam_z])) + cam_pos\n",
        "\n",
        "    return world_pos\n",
        "\n",
        "def project_image(image_file, lat, lng, heading, pitch):\n",
        "    # Load image and depth map\n",
        "    image = cv2.imread(image_file)\n",
        "    depth_file = image_file.replace('.jpg', '_depth.npy')\n",
        "    depth = np.load(depth_file)\n",
        "\n",
        "    # Convert lat, lng to ECEF coordinates\n",
        "    x, y, z = latlng_to_ecef(lat, lng)\n",
        "\n",
        "    # Calculate rotation matrix\n",
        "    R = calc_rotation_matrix(heading, pitch)\n",
        "\n",
        "    # Project each pixel into 3D space\n",
        "    points = []\n",
        "    for y in range(image.shape[0]):\n",
        "        for x in range(image.shape[1]):\n",
        "            world_pos = project_pixel((x, y), depth, fov, R)\n",
        "            points.append((world_pos[0], world_pos[1], world_pos[2], image[y, x, 0], image[y, x, 1], image[y, x, 2]))\n",
        "\n",
        "    # Convert points to NumPy array\n",
        "    points = np.array(points)\n",
        "\n",
        "    return points\n",
        "\n",
        "def get_depth_map(image_file, extractor, model):\n",
        "    # Load image and convert to RGB\n",
        "    image = cv2.imread(image_file)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize image to match model input size\n",
        "    inputs = cv2.resize(image, (640, 384))\n",
        "\n",
        "    # Normalize image pixel values to [0, 1]\n",
        "    inputs = cv2.normalize(inputs, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "    # Convert inputs to PyTorch tensor\n",
        "    inputs = torch.from_numpy(inputs).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    # Extract features from inputs\n",
        "    with torch.no_grad():\n",
        "        features = extractor(inputs)['pixel_values']\n",
        "\n",
        "    # Estimate depth map from features\n",
        "    with torch.no_grad():\n",
        "        outputs = model(features)['log_prediction']\n",
        "        depth_map = torch.squeeze(outputs, dim=0).exp().numpy()\n",
        "\n",
        "    # Resize depth map to match original image size\n",
        "    depth_map = cv2.resize(depth_map, (image.shape[1], image.shape[0]))\n",
        "\n",
        "    return depth_map\n",
        "\n"
      ],
      "metadata": {
        "id": "5YQbc1zRJCf8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Google Street View image\n",
        "image_url = get_streetview_image_url(coordinates[0], coordinates[1], GOOGLE_API_KEY, 120)\n",
        "if image_url is not None:\n",
        "    image_filepath = \"/content/streetview_image_120.jpg\"\n",
        "    # download_image(image_url, image_filepath)\n",
        "\n",
        "    # Segment the image using the Hugging Face model\n",
        "    segmentation_result = segment_image(image_filepath, HF_MODEL)\n",
        "    print(segmentation_result)\n",
        "\n",
        "    # Create a false color image and display the results\n",
        "    false_color_image, label_colors = create_false_color_image(segmentation_result, (640, 640))\n",
        "    false_color_image.show()\n",
        "\n",
        "    # Save the classes and their colors to a JSON file\n",
        "    json_filepath = \"classes.json\"\n",
        "    save_classes_to_json(label_colors, json_filepath)\n",
        "else:\n",
        "    print(\"Failed to download the Street View image.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsRfsrD0M2Ph",
        "outputId": "4f0d2957-e6d0-4176-bbb8-49e980fa5f67"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': None, 'label': 'wall', 'mask': <PIL.Image.Image image mode=L size=640x640 at 0x7FBF5A5B75E0>}, {'score': None, 'label': 'building', 'mask': <PIL.Image.Image image mode=L size=640x640 at 0x7FBF5A5B7190>}, {'score': None, 'label': 'sky', 'mask': <PIL.Image.Image image mode=L size=640x640 at 0x7FBF5A5B7B80>}, {'score': None, 'label': 'tree', 'mask': <PIL.Image.Image image mode=L size=640x640 at 0x7FBF5A5B7100>}, {'score': None, 'label': 'road', 'mask': <PIL.Image.Image image mode=L size=640x640 at 0x7FBF5A5B7F70>}, {'score': None, 'label': 'sidewalk', 'mask': <PIL.Image.Image image mode=L size=640x640 at 0x7FBF5A5B7AF0>}, {'score': None, 'label': 'person', 'mask': <PIL.Image.Image image mode=L size=640x640 at 0x7FBF5A5B7C40>}, {'score': None, 'label': 'car', 'mask': <PIL.Image.Image image mode=L size=640x640 at 0x7FC0E9730C10>}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_filepaths = []\n",
        "headings = range(0, 360, 30)\n",
        "\n",
        "for heading in headings:\n",
        "    image_url = get_streetview_image_url(coordinates[0], coordinates[1], GOOGLE_API_KEY, heading)\n",
        "    if image_url is not None:\n",
        "        image_filepath = f\"streetview_image_{heading}.jpg\"\n",
        "        download_image(image_url, image_filepath)\n",
        "        image_filepaths.append(image_filepath)\n"
      ],
      "metadata": {
        "id": "nKt1YRFEROyA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "\n",
        "def create_point_cloud(segmentation_results, lat, lng, heading, pitch, depth_map, fov):\n",
        "    # Convert lat, lng to ECEF coordinates\n",
        "    x, y, z = latlng_to_ecef(lat, lng)\n",
        "\n",
        "    # Calculate rotation matrix\n",
        "    R = calc_rotation_matrix(heading, pitch)\n",
        "\n",
        "    print(R)\n",
        "\n",
        "    # Initialize an empty point cloud\n",
        "    point_cloud = o3d.geometry.PointCloud()\n",
        "\n",
        "    for result in segmentation_results:\n",
        "        mask = np.array(result['mask'])\n",
        "        label = result['label']\n",
        "        for y in range(mask.shape[0]):\n",
        "            for x in range(mask.shape[1]):\n",
        "                if mask[y, x]:\n",
        "                    world_pos = project_pixel((x, y), depth_map, fov, R)\n",
        "                    point = world_pos + np.array([x, y, z])\n",
        "                    point_cloud.points.append(point)\n",
        "\n",
        "    return point_cloud\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoDjb9suWNxr",
        "outputId": "753f9349-ed4c-45ce-918d-693b19f2870e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get depth map and segmentation image\n",
        "image_filepath = '/content/streetview_image_120.jpg'\n",
        "segmentation_result = segment_image(image_filepath, \"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
        "depth_result =  depth_image(image_filepath, \"Intel/dpt-large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhXVbSY8a9xi",
        "outputId": "c6d64a2e-3d45-48ce-bfa4-1a5cb9d6535a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the depth_result variable\n",
        "# print(depth_result)"
      ],
      "metadata": {
        "id": "2_B-oszqX7Rv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a point cloud from segmented data\n",
        "depth_map = np.array(depth_result['depth'])\n",
        "point_cloud = create_point_cloud(segmentation_result, coordinates[0], coordinates[1], 120, 0, depth_map, 90)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JuAR862UBWf",
        "outputId": "a471b2c7-39c2-44ae-9d2f-bd139421bb94"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-bf8c1ce2208f>:89: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
            "  x, y, z = pyproj.transform(wgs84, ecef, lng, lat, 0, radians=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.5       -0.8660254  0.       ]\n",
            " [ 0.8660254 -0.5        0.       ]\n",
            " [ 0.         0.         1.       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(  point_cloud.points[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT3bCQVpaarM",
        "outputId": "2caa815f-02c8-43a1-a258-26d50173c512"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  9.02744175 179.25141508          inf]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the point cloud as a PLY file\n",
        "o3d.io.write_point_cloud(\"point_cloud.ply\", point_cloud)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LP0TYuRYQwN",
        "outputId": "ae472eec-3ea1-4bf4-9574-c8e408bc4f47"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(point_cloud)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG-_0HU-YfXG",
        "outputId": "0474707a-5966-4ffa-8e68-361043b19632"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PointCloud with 409600 points.\n"
          ]
        }
      ]
    }
  ]
}