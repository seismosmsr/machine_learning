{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMonYax3knhI9PJVLtEMYbo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1cd2715e22f41c9bc38192440ac031a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db79a62fe1974c849876ca0abbe123cb",
              "IPY_MODEL_831b4199936744d9ae02a4df5a22e304",
              "IPY_MODEL_960430ad894e4ad08e29dcbd5f83a6f2"
            ],
            "layout": "IPY_MODEL_5fdcc937ed664a5b8338fe499ba56723"
          }
        },
        "db79a62fe1974c849876ca0abbe123cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bddb23e58c4f405b885a7fa31d5752d9",
            "placeholder": "​",
            "style": "IPY_MODEL_c7675674b57d4bbd8435999f7089e5ea",
            "value": "  2%"
          }
        },
        "831b4199936744d9ae02a4df5a22e304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f97588a685af40f6bc656a88316bdf03",
            "max": 693,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35fc98b9f69343dc820ca47484b20115",
            "value": 18
          }
        },
        "960430ad894e4ad08e29dcbd5f83a6f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55560fa66c7940228b44114884ce8480",
            "placeholder": "​",
            "style": "IPY_MODEL_a2c61fe89b324e829837cd36c53137c6",
            "value": " 17/693 [00:35&lt;23:48,  2.11s/it]"
          }
        },
        "5fdcc937ed664a5b8338fe499ba56723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bddb23e58c4f405b885a7fa31d5752d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7675674b57d4bbd8435999f7089e5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f97588a685af40f6bc656a88316bdf03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35fc98b9f69343dc820ca47484b20115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55560fa66c7940228b44114884ce8480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c61fe89b324e829837cd36c53137c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seismosmsr/machine_learning/blob/main/Fine_Tune_Segformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAgvNcvEQRZg",
        "outputId": "1a8ea5a5-dfb6-4c1f-bfba-7a5c3d163eb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets evaluate segments-ai\n",
        "!pip install rasterio scikit-image tensorflow keras gdown\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCL3NDKuGbe6",
        "outputId": "6b17050e-6881-4378-9a93-9fb3724990b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for segments-ai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2022.12.7)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.3)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.22.4)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (8.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.3.7 snuggs-1.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "import rasterio\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "from keras.utils import to_categorical\n",
        "from skimage.util import random_noise"
      ],
      "metadata": {
        "id": "gLb4DHN4Jmuv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SemanticSegmentationDataset(Dataset):\n",
        "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, feature_extractor, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
        "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
        "        self.ann_dir = os.path.join(self.root_dir, \"annotations\")\n",
        "        \n",
        "        # read images\n",
        "        image_file_names = []\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "          image_file_names.extend(files)\n",
        "        self.images = sorted(image_file_names)\n",
        "        \n",
        "        # read annotations\n",
        "        annotation_file_names = []\n",
        "        for root, dirs, files in os.walk(self.ann_dir):\n",
        "          print(files)\n",
        "          annotation_file_names.extend(files)\n",
        "        self.annotations = sorted(annotation_file_names)\n",
        "\n",
        "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.img_dir, self.images[idx]))\n",
        "        annotation = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n",
        "\n",
        "        # make 2D segmentation map (based on 3D one)\n",
        "        # thanks a lot, Stackoverflow: https://stackoverflow.com/questions/61897492/finding-the-number-of-pixels-in-a-numpy-array-equal-to-a-given-color\n",
        "        annotation = np.array(annotation)\n",
        "        annotation_2d = np.zeros((annotation.shape[0], annotation.shape[1]), dtype=np.uint8) # height, width\n",
        "\n",
        "        # Replace intensity values with corresponding labels\n",
        "        for id, label in id2label.items():\n",
        "            annotation[annotation == id] = label\n",
        "\n",
        "        # randomly crop + pad both image and segmentation map to same size\n",
        "        # feature extractor will also reduce labels!\n",
        "        encoded_inputs = self.feature_extractor(image, Image.fromarray(annotation_2d), return_tensors=\"pt\")\n",
        "\n",
        "        for k,v in encoded_inputs.items():\n",
        "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
        "\n",
        "        return encoded_inputs"
      ],
      "metadata": {
        "id": "cdgJ1sh9aNHK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muq2iXPtGOkA",
        "outputId": "8212ba11-768f-4c2a-90e3-a654e743a1ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f4eGmykyiczmNz2VPeNNmQ7aC7q8N_hD\n",
            "To: /content/california_land_use.zip\n",
            "100%|██████████| 2.76G/2.76G [00:28<00:00, 95.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the training dataset\n",
        "url = 'https://drive.google.com/uc?id=1f4eGmykyiczmNz2VPeNNmQ7aC7q8N_hD'\n",
        "output = '/content/california_land_use.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Extract the dataset\n",
        "cwd = os.getcwd()\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(cwd+'/lulc')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the inference dataset\n",
        "#https://drive.google.com/file/d/1mn00JDt51KlhyiVTfPJjRk7Ymzd_zB7n/view?usp=drive_link\n",
        "url = 'https://drive.google.com/uc?id=1mn00JDt51KlhyiVTfPJjRk7Ymzd_zB7n'\n",
        "output = '/content/california_land_use.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Extract the dataset\n",
        "cwd = os.getcwd()\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(cwd+'/sample_data/inference_data/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAkNp4KRHfb3",
        "outputId": "b5ac56b8-af85-4478-cd52-34294e80bb45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mn00JDt51KlhyiVTfPJjRk7Ymzd_zB7n\n",
            "To: /content/california_land_use.zip\n",
            "100%|██████████| 331M/331M [00:03<00:00, 97.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# # specify directories here\n",
        "# input_dir = '/content/lulc/validation/rgbNIR'\n",
        "# output_dir = '/content/lulc_png/validation/rgbNIR'\n",
        "\n",
        "# if not os.path.exists(output_dir):\n",
        "#     os.makedirs(output_dir)\n",
        "\n",
        "# for filename in os.listdir(input_dir):\n",
        "#     if filename.endswith('.tif') or filename.endswith('.tiff'):\n",
        "#         with rasterio.open(os.path.join(input_dir, filename)) as src:\n",
        "#             array = src.read(out=np.zeros((src.count, src.height, src.width), dtype=np.uint8)) # using a uint8 array to hold the raster data\n",
        "#             array = np.moveaxis(array, 0, -1)  # rasterio reads data in [bands, rows, cols] order, we transpose it to [rows, cols, bands] for PIL\n",
        "#         img = Image.fromarray(array)\n",
        "#         out = os.path.splitext(filename)[0] + '.png'\n",
        "#         img.save(os.path.join(output_dir, out))\n",
        "#         print('Converted: ', filename)\n",
        "\n",
        "# print('All tif images have been converted to png.')\n"
      ],
      "metadata": {
        "id": "qCMMa2w1LDSO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # specify directories here\n",
        "# input_dir = '/content/lulc/validation/labels'\n",
        "# output_dir = '/content/lulc_png/validation/labels'\n",
        "\n",
        "# if not os.path.exists(output_dir):\n",
        "#     os.makedirs(output_dir)\n",
        "\n",
        "# for filename in os.listdir(input_dir):\n",
        "#     if filename.endswith('.tif') or filename.endswith('.tiff'):\n",
        "#         with rasterio.open(os.path.join(input_dir, filename)) as src:\n",
        "#             array = src.read(1)  # This reads the first band into a 2D array\n",
        "#             # If data is not uint8, scale or normalize data to 0-255 before conversion\n",
        "#             if array.dtype != np.uint8:\n",
        "#                 array = ((array - array.min()) * (1/(array.max() - array.min()) * 255)).astype('uint8')\n",
        "#         img = Image.fromarray(array)\n",
        "#         out = os.path.splitext(filename)[0] + '.png'\n",
        "#         img.save(os.path.join(output_dir, out))\n",
        "#         print('Converted: ', filename)\n",
        "\n",
        "# print('All tif images have been converted to png.')"
      ],
      "metadata": {
        "id": "ICmYGjoxYpkC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SegformerFeatureExtractor\n",
        "\n",
        "# dataset = load_dataset(\"imagefolder\", data_dir=\"/content/lulc_png/\")\n",
        "feature_extractor = SegformerFeatureExtractor(reduce_labels=True)\n",
        "train_dataset = SemanticSegmentationDataset(root_dir=\"/content/lulc_png/training\", feature_extractor=feature_extractor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9KHWS1dKZ17",
        "outputId": "5487bbf4-0884-476a-ee47-a4e68cd331d4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['6560_3040.png', '8960_4320.png', '6240_4160.png', '4960_7680.png', '7360_4480.png', '5120_2720.png', '4480_5280.png', '7200_4000.png', '5280_8960.png', '6880_6080.png', '4160_7360.png', '6400_3840.png', '3520_7520.png', '7680_3040.png', '5440_3360.png', '7200_3040.png', '5440_7360.png', '3520_3680.png', '5280_8000.png', '3360_3680.png', '4640_7680.png', '8480_5280.png', '2240_6080.png', '5600_3840.png', '5440_5120.png', '6560_7200.png', '4000_4320.png', '3840_2240.png', '7040_5920.png', '6880_3360.png', '6720_7840.png', '3360_6080.png', '4160_1920.png', '7680_4320.png', '4320_5920.png', '4640_3840.png', '8000_6240.png', '2720_6240.png', '4960_6880.png', '5600_2400.png', '5920_5280.png', '7040_5760.png', '4320_4000.png', '6560_4480.png', '4960_3360.png', '4640_1280.png', '5920_4640.png', '4640_7360.png', '3360_7200.png', '3840_2080.png', '5440_4000.png', '7520_6240.png', '8960_4800.png', '5280_5440.png', '6560_2080.png', '5120_1120.png', '6400_4480.png', '6080_5120.png', '4000_7520.png', '5440_3520.png', '8160_5600.png', '2880_3840.png', '5280_8320.png', '4800_4480.png', '5440_3200.png', '6080_6560.png', '6560_6720.png', '4480_6720.png', '8480_5600.png', '8000_5920.png', '7040_3200.png', '3200_5280.png', '6240_2880.png', '5600_6720.png', '3040_6880.png', '4800_7680.png', '4640_5760.png', '6720_6880.png', '5760_6240.png', '2240_5760.png', '8160_4320.png', '4000_3200.png', '4320_2240.png', '5440_8960.png', '6080_8160.png', '4960_5600.png', '5440_7040.png', '5120_4480.png', '7360_3040.png', '6080_7680.png', '5920_3840.png', '4640_6400.png', '3040_4640.png', '6400_2400.png', '6400_7360.png', '4800_7360.png', '4000_6880.png', '5440_6400.png', '4160_5920.png', '6240_2560.png', '8000_4000.png', '3360_7360.png', '8160_6080.png', '4800_3680.png', '6560_4160.png', '2080_5760.png', '4960_4800.png', '4480_5440.png', '8480_4960.png', '7200_3520.png', '7360_4160.png', '4960_8320.png', '6720_3360.png', '7360_6080.png', '2560_6240.png', '6240_5440.png', '4960_3040.png', '4800_5120.png', '4160_6720.png', '6880_4800.png', '5120_6880.png', '6400_7200.png', '5600_4640.png', '7520_4000.png', '4800_1760.png', '6880_5600.png', '6080_4000.png', '8000_6560.png', '7040_7200.png', '6560_6080.png', '5760_2400.png', '8160_4800.png', '4800_7520.png', '5120_1600.png', '6720_6240.png', '5280_6560.png', '6240_2080.png', '2240_5920.png', '6560_5280.png', '6400_8160.png', '7200_6240.png', '6240_5600.png', '8000_6080.png', '5280_3360.png', '4800_3360.png', '3840_3680.png', '4480_3680.png', '3840_6720.png', '4000_2560.png', '4800_5440.png', '6400_6880.png', '4960_4480.png', '4640_8320.png', '8640_4160.png', '5760_8000.png', '5280_4160.png', '8480_5920.png', '4160_2560.png', '5440_2560.png', '5760_4160.png', '7040_5600.png', '6080_7840.png', '6400_7680.png', '5600_8000.png', '3680_2720.png', '2880_6720.png', '7360_7200.png', '6880_5920.png', '8160_6240.png', '4480_3040.png', '3040_6240.png', '3840_6560.png', '7680_4960.png', '7360_3680.png', '8480_5440.png', '5120_4000.png', '9280_4960.png', '7200_7200.png', '3840_3520.png', '4800_1920.png', '4160_4640.png', '7040_4000.png', '4800_2080.png', '5920_2400.png', '2880_6880.png', '4640_5600.png', '6080_4800.png', '5120_7360.png', '4000_6400.png', '3520_5600.png', '5920_6240.png', '2720_5120.png', '7520_3040.png', '8160_3840.png', '3200_7200.png', '8800_5280.png', '4160_4160.png', '3680_3360.png', '4800_1440.png', '7680_4800.png', '6720_5440.png', '3200_3040.png', '5280_7040.png', '7360_7040.png', '5600_6880.png', '5120_8640.png', '3040_3680.png', '6080_7040.png', '4800_4160.png', '6880_3040.png', '6560_4800.png', '7520_4640.png', '6720_7040.png', '8320_4800.png', '5920_7840.png', '6720_5920.png', '3840_3200.png', '6720_3520.png', '7520_6720.png', '4960_4640.png', '7040_5440.png', '4160_3840.png', '7040_6880.png', '7040_4640.png', '2560_6080.png', '6880_4640.png', '6720_2880.png', '3840_5760.png', '3200_5440.png', '6720_3680.png', '4320_1920.png', '5920_6080.png', '6240_1920.png', '4320_7840.png', '7200_4800.png', '3200_3680.png', '7360_4800.png', '9440_4640.png', '4000_2720.png', '8000_4320.png', '5760_2880.png', '4800_7200.png', '6240_6240.png', '6560_3680.png', '3520_5920.png', '6240_4960.png', '5760_3200.png', '6080_2880.png', '3360_2560.png', '4160_4000.png', '4960_1120.png', '2720_6720.png', '8160_3520.png', '5760_8640.png', '7840_3680.png', '7040_6080.png', '4960_5120.png', '5920_8000.png', '4960_3840.png', '6880_3840.png', '6880_4960.png', '3200_6560.png', '8320_4640.png', '6880_3680.png', '6080_5920.png', '8000_3520.png', '4640_5120.png', '6080_3520.png', '3680_4800.png', '3840_5120.png', '5120_7200.png', '7840_6560.png', '9280_4800.png', '5120_8160.png', '4800_6080.png', '8000_4960.png', '5440_4640.png', '4640_6240.png', '4000_6560.png', '9280_5120.png', '6560_2400.png', '4960_960.png', '4160_7200.png', '4480_4000.png', '5600_1600.png', '2720_3200.png', '5440_8480.png', '5120_6080.png', '6720_7360.png', '4960_8160.png', '5920_6400.png', '3680_6880.png', '3520_6240.png', '4320_5600.png', '7040_2560.png', '7360_4320.png', '5280_1440.png', '5280_2880.png', '4640_7520.png', '3200_4320.png', '7680_6400.png', '7840_4960.png', '4320_5280.png', '2080_6240.png', '2560_5600.png', '7520_4800.png', '7840_6080.png', '8320_5280.png', '4960_5280.png', '3680_3520.png', '5120_3840.png', '5600_4480.png', '4160_2400.png', '3200_3840.png', '6880_7680.png', '8640_4480.png', '6560_3520.png', '4320_4480.png', '4800_5760.png', '7520_4480.png', '8320_4480.png', '5600_2560.png', '5760_4800.png', '5920_1760.png', '7200_4480.png', '3360_5920.png', '4480_3200.png', '4640_8000.png', '5120_2080.png', '8000_4800.png', '3360_3040.png', '6080_8480.png', '5600_6400.png', '5920_4320.png', '4160_7680.png', '3360_3200.png', '2880_3360.png', '4800_5920.png', '5600_7200.png', '2080_5600.png', '4640_5280.png', '4480_5760.png', '5440_9120.png', '5920_2080.png', '5920_6560.png', '5600_8640.png', '4160_6400.png', '4960_8480.png', '3840_7360.png', '5280_6400.png', '6880_2880.png', '5120_6720.png', '5440_1920.png', '7520_5120.png', '5920_8160.png', '4160_6560.png', '7360_6400.png', '5120_800.png', '7040_3040.png', '4960_3680.png', '4320_6400.png', '5760_7200.png', '2720_3520.png', '3680_5920.png', '8320_5600.png', '6400_3520.png', '7200_5440.png', '2240_5440.png', '5600_5440.png', '7200_6560.png', '5920_4000.png', '6080_7360.png', '6560_5440.png', '4800_3520.png', '2720_6080.png', '8640_5760.png', '5440_7680.png', '9120_5280.png', '4160_3680.png', '4160_4960.png', '6560_7360.png', '6240_4800.png', '6880_5440.png', '5440_8640.png', '3680_7360.png', '4480_6080.png', '3040_6080.png', '6240_4480.png', '8320_3840.png', '8000_5280.png', '6880_6560.png', '4800_2400.png', '3520_3040.png', '4640_1600.png', '5440_2400.png', '5440_6560.png', '3840_6400.png', '4320_7040.png', '5280_4320.png', '6400_6560.png', '5920_5920.png', '4800_2720.png', '5440_2240.png', '4480_5120.png', '5440_5280.png', '3360_6880.png', '4000_4640.png', '3680_4960.png', '6880_3520.png', '4960_7520.png', '5760_1920.png', '6400_1920.png', '5280_8160.png', '4480_4320.png', '6880_4480.png', '4320_4640.png', '5120_7040.png', '2880_6560.png', '5920_6720.png', '5760_6880.png', '7200_4320.png', '5600_6080.png', '4480_7040.png', '2880_3520.png', '7680_3840.png', '4800_1280.png', '4640_6720.png', '4160_3040.png', '4000_7680.png', '3360_4160.png', '8320_4320.png', '4640_6880.png', '2880_4800.png', '7520_6560.png', '3680_7200.png', '4160_7040.png', '4320_5440.png', '6720_4160.png', '5760_5760.png', '7200_6720.png', '3200_6080.png', '3200_4640.png', '8000_3680.png', '7360_5600.png', '5280_1280.png', '7040_3520.png', '3680_4640.png', '3680_5280.png', '4640_3360.png', '7200_2880.png', '6080_6720.png', '7680_4000.png', '5760_7360.png', '7040_5120.png', '4160_2080.png', '6080_2720.png', '8640_4000.png', '3360_5440.png', '8000_4640.png', '6080_4320.png', '6240_3040.png', '2400_5440.png', '2880_5280.png', '6400_4960.png', '4800_6880.png', '4800_1600.png', '7680_5600.png', '5920_8480.png', '1920_5760.png', '5600_7040.png', '5280_6240.png', '3040_6560.png', '5760_6080.png', '8480_5760.png', '4160_6080.png', '4480_4640.png', '5600_2240.png', '6560_2240.png', '8800_5440.png', '4000_7040.png', '3040_5440.png', '5120_1760.png', '4640_8480.png', '3840_5280.png', '4000_1920.png', '8640_4320.png', '4480_3520.png', '3840_3040.png', '3680_3040.png', '4000_6240.png', '6720_2240.png', '3520_6880.png', '4800_3200.png', '6240_7360.png', '7840_4320.png', '3040_5280.png', '8800_4800.png', '3360_6240.png', '6400_2240.png', '5280_1920.png', '4000_3840.png', '4320_7520.png', '4640_2880.png', '7360_3360.png', '5920_6880.png', '5440_1440.png', '4000_5920.png', '4000_2240.png', '6560_4960.png', '4960_4320.png', '5120_5600.png', '6240_3840.png', '6880_4160.png', '4320_8000.png', '3520_3360.png', '6080_4160.png', '5440_4960.png', '3840_4000.png', '4000_5600.png', '5920_4160.png', '3200_5760.png', '3200_6400.png', '5120_8800.png', '4640_3680.png', '4160_7840.png', '6240_7520.png', '3520_2560.png', '3520_4480.png', '3040_6720.png', '4640_7200.png', '5280_4640.png', '6720_4800.png', '3200_7040.png', '6720_4320.png', '6400_4800.png', '4000_5760.png', '6240_2400.png', '8320_4960.png', '6720_3840.png', '3200_3360.png', '5760_6400.png', '6080_3680.png', '7680_5760.png', '5120_2560.png', '3680_6560.png', '4480_2720.png', '7680_4160.png', '6080_5280.png', '5920_5120.png', '5600_6560.png', '8800_4320.png', '5760_8320.png', '5120_2240.png', '5760_3840.png', '4960_5440.png', '5920_3520.png', '3360_7040.png', '4480_5600.png', '4000_3360.png', '2080_5920.png', '2560_5920.png', '6720_6400.png', '5280_9120.png', '4800_2240.png', '5920_7040.png', '4960_1280.png', '5760_1600.png', '2880_5600.png', '6720_3200.png', '3520_4640.png', '4960_1760.png', '5440_6080.png', '5760_5280.png', '3200_4800.png', '5280_8480.png', '6560_3200.png', '5760_4480.png', '4800_8160.png', '2720_5600.png', '9120_4800.png', '4640_2400.png', '3360_4960.png', '4160_5120.png', '4160_4800.png', '4320_8320.png', '3840_4480.png', '7520_5760.png', '3360_4320.png', '3040_2880.png', '4160_5600.png', '4960_4160.png', '7520_3520.png', '5280_4480.png', '2240_5280.png', '4000_3040.png', '5280_7840.png', '4960_6400.png', '5600_3200.png', '4480_1600.png', '4160_5440.png', '6560_6400.png', '3200_6720.png', '6720_7520.png', '3360_6720.png', '4320_3360.png', '4320_3680.png', '5760_5120.png', '3040_4960.png', '5600_8960.png', '7680_6720.png', '4640_4640.png', '5120_5920.png', '3680_5760.png', '7840_5440.png', '4800_4320.png', '4960_5920.png', '2880_4000.png', '7360_3520.png', '5600_7680.png', '7680_4640.png', '4960_7040.png', '5920_2560.png', '2880_5440.png', '3520_5120.png', '5120_960.png', '5760_6720.png', '7200_4960.png', '7680_3680.png', '1920_6080.png', '5440_4800.png', '3520_4000.png', '5600_1760.png', '3040_7040.png', '5280_1120.png', '5760_3360.png', '7200_3840.png', '3360_3840.png', '6720_5120.png', '5760_2240.png', '3040_3200.png', '5120_4160.png', '5760_1440.png', '8160_4480.png', '8960_5440.png', '3840_5440.png', '3520_6080.png', '3520_2400.png', '2560_3360.png', '6240_8320.png', '2080_6080.png', '6080_1920.png', '4640_5920.png', '3360_4640.png', '5920_5440.png', '4320_1760.png', '7840_3840.png', '5920_3680.png', '5280_2560.png', '5280_7520.png', '5600_8800.png', '4480_6880.png', '8160_5280.png', '6880_4000.png', '5280_5280.png', '4480_2880.png', '9120_4640.png', '5600_5120.png', '5920_3360.png', '5600_3040.png', '5280_5600.png', '7840_5920.png', '4800_4000.png', '5280_5760.png', '7200_5280.png', '7520_5600.png', '2880_5920.png', '4640_5440.png', '3680_6400.png', '6560_6240.png', '4640_3040.png', '5600_3680.png', '2560_5120.png', '6560_2880.png', '4160_6880.png', '8960_4480.png', '5440_8320.png', '5600_4160.png', '5120_4640.png', '6400_7040.png', '3680_3200.png', '5920_2720.png', '4320_5760.png', '8160_6400.png', '8000_3360.png', '7840_4160.png', '6880_6240.png', '3200_3200.png', '8800_4160.png', '4640_4480.png', '3840_4160.png', '3840_4800.png', '4160_2720.png', '4960_3200.png', '8640_5600.png', '3520_3200.png', '4480_7520.png', '7520_5440.png', '7520_6400.png', '6080_4960.png', '2720_5760.png', '6240_6560.png', '5120_3200.png', '3840_7840.png', '3360_2720.png', '7200_5120.png', '5280_6880.png', '6240_2720.png', '6880_7520.png', '6080_3040.png', '6720_2560.png', '7360_3200.png', '6720_5760.png', '6880_6720.png', '4000_3520.png', '7040_7360.png', '3680_7520.png', '4960_6720.png', '3200_6880.png', '7200_2720.png', '3360_3360.png', '4800_3840.png', '6720_5600.png', '8800_5600.png', '6240_6880.png', '7360_6880.png', '6080_7520.png', '2400_6240.png', '6880_7040.png', '5280_3680.png', '7680_4480.png', '8960_4960.png', '3680_3680.png', '4800_3040.png', '3840_4320.png', '6720_6560.png', '3360_5280.png', '5280_1600.png', '6720_4960.png', '6240_5920.png', '5600_4960.png', '4640_4800.png', '4800_5280.png', '3680_4000.png', '4000_4160.png', '2880_3040.png', '4320_3200.png', '4960_7360.png', '6080_2400.png', '4640_2560.png', '5760_3680.png', '7840_6720.png', '2400_5760.png', '4800_7840.png', '3680_5440.png', '2720_6560.png', '6880_2720.png', '3840_7520.png', '6400_5600.png', '4000_7840.png', '6080_3360.png', '6080_7200.png', '6240_8000.png', '4160_2880.png', '6240_6400.png', '4960_3520.png', '7680_6080.png', '5120_4800.png', '6400_3360.png', '6560_8000.png', '3360_4800.png', '3520_7040.png', '2560_5440.png', '8320_4160.png', '4480_3840.png', '5280_2240.png', '6240_4640.png', '3200_5120.png', '4000_3680.png', '6080_6240.png', '2560_3520.png', '1760_5920.png', '5120_1280.png', '4480_7200.png', '4800_8320.png', '2720_3680.png', '4960_2880.png', '4640_6560.png', '4640_2720.png', '4640_3520.png', '6560_2560.png', '9120_4480.png', '4800_4960.png', '5600_5280.png', '2880_4960.png', '3040_3040.png', '5440_3680.png', '3520_4800.png', '2400_5600.png', '4800_8000.png', '4800_5600.png', '4960_5760.png', '4640_2240.png', '3360_6400.png', '6240_6720.png', '7840_4800.png', '3840_3360.png', '3840_7040.png', '8800_4960.png', '4480_6400.png', '6080_4640.png', '4000_5280.png', '7040_5280.png', '3040_5120.png', '6400_6720.png', '3040_4800.png', '7680_5440.png', '6720_4000.png', '5600_7520.png', '7200_6880.png', '6240_6080.png', '4480_4800.png', '7200_5920.png', '7040_3360.png', '7680_6240.png', '5600_4320.png', '6400_2720.png', '4000_2880.png', '3680_2240.png', '7040_4160.png', '3840_2400.png', '5920_8320.png', '6080_5440.png', '4480_3360.png', '5920_1600.png', '8640_5280.png', '6560_7840.png', '3680_6080.png', '4640_2080.png', '9280_4640.png', '5760_4320.png', '6240_3200.png', '3360_5120.png', '5600_5920.png', '4160_8000.png', '4320_2720.png', '4000_4800.png', '4320_7200.png', '5760_1760.png', '9120_4960.png', '5600_1280.png', '4640_1440.png', '5600_1920.png', '2880_3200.png', '5760_5920.png', '7200_5600.png', '8960_4640.png', '7520_3360.png', '8000_5760.png', '8640_4640.png', '3680_4320.png', '7200_4160.png', '7360_5760.png', '3680_6720.png', '4800_4640.png', '6400_3040.png', '5920_7520.png', '7040_7520.png', '7360_5920.png', '5600_4000.png', '5920_7680.png', '5120_4320.png', '7200_6400.png', '2400_5920.png', '5600_2880.png', '5280_5920.png', '8160_5760.png', '2560_6400.png', '4000_4960.png', '5440_4160.png', '4800_8640.png', '8320_5760.png', '6720_7680.png', '5120_4960.png', '5120_1440.png', '8480_3840.png', '6240_3360.png', '7840_4480.png', '8800_4480.png', '5440_5760.png', '6080_5760.png', '4320_5120.png', '5440_7200.png', '4320_4320.png', '4960_2720.png', '4320_7680.png', '7520_4960.png', '4480_7360.png', '7040_3840.png', '5120_8000.png', '1920_5920.png', '6560_5760.png', '5760_6560.png', '4160_1760.png', '3040_5760.png', '5120_7680.png', '6880_5280.png', '8160_5440.png', '4000_2400.png', '5120_7520.png', '4640_1920.png', '8160_4960.png', '8160_4000.png', '4160_7520.png', '6400_4640.png', '4160_4320.png', '3840_4640.png', '6560_7040.png', '4160_4480.png', '5120_8320.png', '8480_4800.png', '6400_5920.png', '7200_6080.png', '5760_2560.png', '4960_8000.png', '6720_4480.png', '4000_6720.png', '8640_4800.png', '6400_5280.png', '2880_3680.png', '4640_7040.png', '6400_7840.png', '5760_7840.png', '6240_7680.png', '5920_4960.png', '6400_5760.png', '5440_1760.png', '3520_6400.png', '7520_5920.png', '4960_7200.png', '5280_7360.png', '2720_5280.png', '2240_5600.png', '3360_5760.png', '7040_3680.png', '3360_4480.png', '5440_7520.png', '4960_2560.png', '6240_5760.png', '3520_5440.png', '3840_3840.png', '4640_4160.png', '5280_7680.png', '7840_5600.png', '2400_6400.png', '6080_5600.png', '5280_1760.png', '4480_8320.png', '5600_3360.png', '2880_6080.png', '8320_5440.png', '7520_4160.png', '8960_5280.png', '3200_5920.png', '4000_5440.png', '3520_6560.png', '7360_5120.png', '3360_5600.png', '2880_6240.png', '5120_7840.png', '5440_8800.png', '6240_4000.png', '8320_4000.png', '5120_2400.png', '4640_7840.png', '9440_4960.png', '3200_6240.png', '8320_5920.png', '5920_4800.png', '7520_3200.png', '6880_2400.png', '8320_5120.png', '7680_6560.png', '5760_4960.png', '5440_7840.png', '3840_2880.png', '7360_6720.png', '6560_4320.png', '3200_2880.png', '4960_6240.png', '7840_5760.png', '4800_4800.png', '3040_5600.png', '4160_3200.png', '6080_1760.png', '3520_2720.png', '5120_6400.png', '5440_5600.png', '7680_3520.png', '8160_4160.png', '3520_2880.png', '5600_6240.png', '4320_2560.png', '5440_4320.png', '6240_3680.png', '5280_7200.png', '5280_960.png', '5280_2400.png', '3520_5280.png', '5120_5280.png', '3200_3520.png', '5760_5440.png', '3360_2880.png', '4640_6080.png', '5600_5600.png', '4320_2880.png', '8480_5120.png', '5280_8640.png', '5920_8640.png', '5440_2880.png', '5120_8480.png', '7040_6560.png', '6400_5440.png', '3040_3520.png', '6560_4640.png', '2560_6560.png', '4480_6560.png', '6720_2720.png', '2400_5280.png', '5760_3520.png', '4320_6880.png', '3840_5920.png', '7200_7360.png', '4800_6240.png', '6240_7200.png', '3520_7360.png', '7360_2880.png', '5280_8800.png', '4320_2080.png', '7040_6400.png', '4480_2400.png', '5920_3200.png', '5440_4480.png', '7520_4320.png', '5280_6080.png', '6080_6080.png', '5440_6880.png', '6080_8000.png', '4960_2240.png', '5920_1920.png', '6240_1760.png', '8480_4480.png', '5920_7360.png', '4160_3360.png', '6400_7520.png', '5280_2720.png', '5440_2080.png', '2720_5440.png', '7840_6240.png', '7840_3200.png', '4960_4000.png', '4320_3840.png', '4640_8160.png', '5600_8160.png', '4000_6080.png', '5760_7520.png', '6560_4000.png', '8480_4000.png', '3040_3840.png', '4000_7360.png', '4960_7840.png', '6880_2560.png', '7520_6880.png', '3360_6560.png', '4640_4320.png', '7680_3200.png', '5120_6560.png', '7040_2720.png', '5120_3680.png', '6080_2080.png', '3840_6880.png', '4640_4960.png', '6240_4320.png', '3520_7200.png', '3360_3520.png', '4320_3040.png', '6240_7040.png', '6560_5120.png', '3360_4000.png', '3680_7040.png', '4640_4000.png', '3520_4160.png', '4320_6560.png', '4480_1760.png', '2880_5760.png', '7680_5120.png', '5280_2080.png', '2720_5920.png', '7360_4640.png', '2400_6080.png', '5440_6720.png', '6560_3840.png', '3840_2720.png', '4320_7360.png', '6880_6880.png', '4960_2400.png', '5120_3040.png', '8000_5120.png', '8000_5440.png', '5120_5440.png', '5440_3840.png', '7680_3360.png', '3040_6400.png', '5920_7200.png', '4960_6560.png', '5760_3040.png', '4640_3200.png', '4800_6400.png', '3200_4160.png', '4160_6240.png', '5920_5600.png', '5760_2720.png', '4800_6720.png', '4800_6560.png', '5120_8960.png', '4480_1440.png', '5440_8160.png', '6080_2240.png', '2560_5280.png', '7840_3360.png', '7040_4960.png', '5760_2080.png', '5760_5600.png', '4800_1120.png', '5280_4800.png', '8000_5600.png', '3040_5920.png', '3200_4000.png', '6400_6240.png', '3680_5120.png', '4480_2240.png', '4320_6080.png', '4160_2240.png', '6080_2560.png', '4480_8160.png', '4160_5760.png', '5440_8000.png', '5280_3840.png', '2880_6400.png', '6400_2080.png', '6720_6080.png', '5760_4000.png', '6720_3040.png', '7200_3680.png', '4320_4160.png', '5120_2880.png', '4800_2880.png', '3840_6240.png', '4480_4480.png', '3040_4160.png', '8800_5120.png', '5920_4480.png', '7840_6400.png', '4000_4000.png', '4480_7680.png', '6400_3680.png', '7840_4000.png', '6880_3200.png', '5280_4000.png', '8160_5120.png', '5440_3040.png', '8480_4160.png', '4160_8160.png', '8640_5120.png', '3680_2880.png', '8480_4320.png', '7680_6880.png', '7040_6240.png', '3520_3520.png', '4320_4800.png', '7200_3200.png', '4480_2560.png', '7040_4480.png', '6880_5120.png', '3680_5600.png', '8000_4160.png', '3200_4960.png', '4320_3520.png', '6560_6880.png', '8320_6080.png', '5120_3360.png', '3840_5600.png', '4960_4960.png', '6080_6400.png', '6240_8160.png', '5600_8480.png', '6240_5120.png', '6880_5760.png', '5280_3040.png', '3680_7680.png', '6240_7840.png', '3680_2560.png', '5600_3520.png', '6080_4480.png', '6560_6560.png', '8160_3680.png', '4800_7040.png', '4320_4960.png', '5600_1440.png', '4000_4480.png', '4320_6720.png', '5120_5120.png', '6560_7680.png', '3200_2720.png', '4960_1440.png', '6560_5920.png', '7360_4960.png', '4320_1600.png', '5600_4800.png', '3520_4320.png', '6080_3840.png', '3520_6720.png', '4480_1920.png', '4960_8640.png', '7680_5280.png', '2720_6400.png', '7520_5280.png', '2240_6240.png', '5600_7360.png', '4480_4160.png', '7360_4000.png', '5760_7680.png', '7360_6560.png', '5440_6240.png', '9120_5120.png', '3680_2400.png', '7520_6080.png', '8000_6400.png', '8640_5440.png', '3680_6240.png', '4480_5920.png', '6880_7360.png', '5280_6720.png', '8480_4640.png', '7360_5280.png', '3840_7680.png', '2560_5760.png', '7040_4800.png', '6560_7520.png', '6400_4000.png', '5120_3520.png', '5600_9120.png', '5920_3040.png', '6080_3200.png', '5120_6240.png', '6720_4640.png', '4000_7200.png', '6400_6400.png', '3520_3840.png', '6720_7200.png', '6560_2720.png', '7520_3680.png', '4000_5120.png', '3520_5760.png', '7840_4640.png', '4320_2400.png', '4960_6080.png', '8960_5120.png', '5440_1120.png', '3520_4960.png', '5280_4960.png', '6400_4160.png', '4960_8800.png', '5440_2720.png', '3040_4320.png', '5920_2880.png', '8160_5920.png', '5760_4640.png', '7040_2880.png', '5760_7040.png', '5760_8160.png', '4480_6240.png', '6400_2880.png', '6880_7200.png', '4640_1760.png', '3200_5600.png', '6560_5600.png', '7360_6240.png', '6880_6400.png', '4800_8480.png', '4320_6240.png', '2880_5120.png', '8000_3840.png', '5920_2240.png', '4960_1600.png', '6720_5280.png', '5760_8800.png', '4960_2080.png', '5600_5760.png', '4480_7840.png', '7200_7040.png', '4160_3520.png', '8640_4960.png', '7360_3840.png', '6880_4320.png', '4480_8000.png', '5280_3200.png', '6400_3200.png', '6720_2400.png', '2400_5120.png', '5600_2080.png', '3840_7200.png', '3040_4000.png', '4000_8000.png', '7040_7040.png', '5600_7840.png', '6080_6880.png', '5600_2720.png', '6240_2240.png', '7200_5760.png', '6240_3520.png', '8000_4480.png', '6720_6720.png', '4480_2080.png', '5920_5760.png', '5760_8480.png', '3840_4960.png', '7520_3840.png', '6400_8000.png', '7840_3520.png', '5120_5760.png', '5600_8320.png', '3200_4480.png', '4320_8160.png', '3840_2560.png', '6400_2560.png', '7840_5280.png', '6080_8320.png', '9440_4800.png', '5440_1600.png', '5440_5920.png', '5120_1920.png', '7200_4640.png', '8160_4640.png', '2720_4960.png', '6400_6080.png', '7200_3360.png', '5280_5120.png', '2720_3360.png', '3680_3840.png', '5440_5440.png', '9280_4480.png', '6240_5280.png', '7040_6720.png', '4960_1920.png', '6560_3360.png', '4160_5280.png', '5440_1280.png', '7840_5120.png', '6400_4320.png', '8800_4640.png', '3840_6080.png', '3040_3360.png', '7360_5440.png', '7040_4320.png', '3680_4480.png', '5280_3520.png', '4800_2560.png', '4480_4960.png', '7680_5920.png', '8320_3680.png', '6400_5120.png', '7520_7040.png', '3040_4480.png', '4000_2080.png', '3680_4160.png']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JDpNNJKvaRfz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset['train']['image']\n",
        "id = [i for i in [0,31,63,95,127,159,191,223,255]]\n",
        "string = [str(i) for i in range(9)]"
      ],
      "metadata": {
        "id": "VycU_EtLKswL"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2color = {id: color for id, color in zip(id,string)}\n",
        "label2id = {label: id for id, label in id2color.items()}\n",
        "id2label = {id: label for id, label in id2color.items()}\n",
        "label2id\n",
        "id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ND0TR16RC_g",
        "outputId": "64c83c03-4c41-4df4-9bda-e72d363d904a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '0',\n",
              " 31: '1',\n",
              " 63: '2',\n",
              " 95: '3',\n",
              " 127: '4',\n",
              " 159: '5',\n",
              " 191: '6',\n",
              " 223: '7',\n",
              " 255: '8'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81dWx1WBqIos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acd5f35-2525-43a9-e75a-2db5829e9e62"
      },
      "source": [
        "from transformers import SegformerForSemanticSegmentation\n",
        "\n",
        "# define model\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\",\n",
        "                                                         num_labels=9, \n",
        "                                                         id2label=id2label, \n",
        "                                                         label2id=label2id,\n",
        ")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.batch_norm.running_var', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.classifier.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.0.proj.weight', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.classifier.bias', 'decode_head.batch_norm.running_mean']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"mean_iou\")"
      ],
      "metadata": {
        "id": "07cY3C8SVmiw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9kuSZNuV-lH",
        "outputId": "bd642cce-0224-41a3-8e21-2be348d26ebe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 2770\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 190\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "MrQAVIHHV7S-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enumerate(tqdm(train_dataloader))"
      ],
      "metadata": {
        "id": "bavNZdGhceNW"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
        "# move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):  # loop over the dataset multiple times\n",
        "   print(\"Epoch:\", epoch)\n",
        "   for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss, logits = outputs.loss, outputs.logits\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # evaluate\n",
        "        with torch.no_grad():\n",
        "          upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "          predicted = upsampled_logits.argmax(dim=1)\n",
        "          \n",
        "          # note that the metric expects predictions + labels as numpy arrays\n",
        "          metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # let's print loss and metrics every 100 batches\n",
        "        if idx % 100 == 0:\n",
        "          # we use _compute for now which fixes an issue in speed\n",
        "          # see this Github thread for more info: \n",
        "          metrics = metric.compute(num_labels = 9,ignore_index=True)\n",
        "\n",
        "          print(\"Loss:\", loss.item())\n",
        "          print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
        "          print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "e1cd2715e22f41c9bc38192440ac031a",
            "db79a62fe1974c849876ca0abbe123cb",
            "831b4199936744d9ae02a4df5a22e304",
            "960430ad894e4ad08e29dcbd5f83a6f2",
            "5fdcc937ed664a5b8338fe499ba56723",
            "bddb23e58c4f405b885a7fa31d5752d9",
            "c7675674b57d4bbd8435999f7089e5ea",
            "f97588a685af40f6bc656a88316bdf03",
            "35fc98b9f69343dc820ca47484b20115",
            "55560fa66c7940228b44114884ce8480",
            "a2c61fe89b324e829837cd36c53137c6"
          ]
        },
        "id": "SkKx8UuTVuqC",
        "outputId": "4df620ca-1723-4ae5-f05f-b4e29ac338fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/693 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1cd2715e22f41c9bc38192440ac031a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/huggingface/modules/datasets_modules/metrics/mean_iou/927b58f57da3f4b6e385e47d8a4b3947ee3f7cfcdba9b9359eba2ada2ed6b951/mean_iou.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  all_acc = total_area_intersect.sum() / total_area_label.sum()\n",
            "/root/.cache/huggingface/modules/datasets_modules/metrics/mean_iou/927b58f57da3f4b6e385e47d8a4b3947ee3f7cfcdba9b9359eba2ada2ed6b951/mean_iou.py:259: RuntimeWarning: invalid value encountered in true_divide\n",
            "  acc = total_area_intersect / total_area_label\n",
            "/root/.cache/huggingface/modules/datasets_modules/metrics/mean_iou/927b58f57da3f4b6e385e47d8a4b3947ee3f7cfcdba9b9359eba2ada2ed6b951/mean_iou.py:262: RuntimeWarning: Mean of empty slice\n",
            "  metrics[\"mean_accuracy\"] = np.nanmean(acc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: nan\n",
            "Mean_iou: 0.0\n",
            "Mean accuracy: nan\n"
          ]
        }
      ]
    }
  ]
}