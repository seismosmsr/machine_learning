{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to use the Kaggle API from Colab",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seismosmsr/machine_learning/blob/main/kaggle_api_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "DIf2PtekjPtY"
      },
      "cell_type": "markdown",
      "source": [
        "# Installing the [Kaggle API](https://github.com/Kaggle/kaggle-api) in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just to ensure we've got our requirements met. This also works if you choose to run things on the kaggle back end (code for pushing notebook to auto submit from kaggle at bottom of this script)."
      ],
      "metadata": {
        "id": "_mrZmulPFh04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#THis is a new important line"
      ],
      "metadata": {
        "id": "tb-UCvof6yEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OppyMnCuWjzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae581cc8-0d9c-4240-ad35-821b555a001c"
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!conda install -y gdown"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.63.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "/bin/bash: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hMY4CFezjcG-"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticating with Kaggle using kaggle.json\n",
        "\n",
        "Navigate to https://www.kaggle.com. Then go to the [Account tab of your user profile](https://www.kaggle.com/me/account) and select Create API Token. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
        "\n",
        "Then run the cell below to upload kaggle.json to your Colab runtime."
      ]
    },
    {
      "metadata": {
        "id": "0HtGf0HEXEa5",
        "outputId": "a2e14b78-903f-4293-b3bc-9387419108ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "import gdown\n",
        "#This is my personal kaggle json. If you run this and don't switch it out, you'll be running as me.\n",
        "!gdown --id 1sD1x-nf2nXNNDFD3zdPKvOKM2wGSWcSP\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sD1x-nf2nXNNDFD3zdPKvOKM2wGSWcSP\n",
            "To: /content/kaggle.json\n",
            "\r  0% 0.00/69.0 [00:00<?, ?B/s]\r100% 69.0/69.0 [00:00<00:00, 94.2kB/s]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HMk7Zz4ZkpCe"
      },
      "cell_type": "markdown",
      "source": [
        "# Using the Kaggle API\n",
        "\n",
        "For a more complete list of what you can do with the API, visit https://github.com/Kaggle/kaggle-api."
      ]
    },
    {
      "metadata": {
        "id": "dNke00r6ig3h"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading a dataset"
      ]
    },
    {
      "metadata": {
        "id": "Aojvqv8Gaf8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25acb1d-c36f-4e37-d5fd-6a33ee5d0a39"
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c covid-19-risk-2022"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading covid-19-risk-2022.zip to /content\n",
            " 98% 289M/296M [00:02<00:00, 124MB/s]\n",
            "100% 296M/296M [00:02<00:00, 135MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip the data and take a first glance."
      ],
      "metadata": {
        "id": "CusXRKTUGPSq"
      }
    },
    {
      "metadata": {
        "id": "DsyV01gDaxls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdef64d2-f4bd-4ec7-adb0-d9e325377b49"
      },
      "cell_type": "code",
      "source": [
        "!unzip train.csv.zip\n",
        "\n",
        "!unzip test.csv.zip\n",
        "\n",
        "!unzip train_small.csv.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open train.csv.zip, train.csv.zip.zip or train.csv.zip.ZIP.\n",
            "unzip:  cannot find or open test.csv.zip, test.csv.zip.zip or test.csv.zip.ZIP.\n",
            "unzip:  cannot find or open train_small.csv.zip, train_small.csv.zip.zip or train_small.csv.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "L7ZmFbHybsPk",
        "outputId": "894fb2e9-4989-4fcf-cdcf-38be01831332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "\n",
        "train_small = pd.read_csv('/content/train_small.csv')\n",
        "# train = pd.read_csv('/content/train.csv')\n",
        "# test = pd.read_csv('/content/test.csv')\n",
        "train_small.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8e91030cebff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_small\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train_small.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# train = pd.read_csv('/content/train.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test = pd.read_csv('/content/test.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_small.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we've got a case_month, res_state, state_fips_code, res_county, county_fips_code, age_group, sex, race, ethnicity, lots more stuff. Some issues I can think of right away and which we've gone over in class, are the completeness of the data. A couple of approaches I've taken to these issues in the past are usually types of imputation. My initial thinking was to just randomly replace missing data with valid data. Another option would be to use clustering or random forest to create 'informed' imputed postiions. A third option would be to just not use the missing data and to try and maybe group 'types' of missing data together, then maximise the training set by 'grouping' the classes. \n",
        "\n",
        "Any which way, the method that we use to deal with NA's is going to be important. If we use a pure random imputation, this ..may.. be able to keep us unbiased (makes some assumptions about underlying distribution of missing data), but we definately stand a chance of losing information if we can't some how randomize the way the data is missing. One option might be to instead of only randomly imputing to that data once, we could do it many many times. If we do this many times, we could actually re-use some of our training data. We might think of this as similar to 'fuzzing' our data in deep learning (basic data augmetation).\n",
        "\n",
        "If we use a systematic approach (like train an algorithm to impute for us), one issue we'll face is that it could bias our new 'imputed' training dataset. What I mean by this, is that our NA's may not be randomly distributed throughout our training dataset. Some hospitals or agencys may get some data and not others. Maybe some states are better or worse at their quality of reporting. If we rely on an algorthmic approach, we'll effectively be baking whatever biases exist in the meta context of our data into our data. We don't neccessarily want to do that.\n",
        "\n",
        "Finally, we could also find a way to 'ignore' NA's while losing as little data as possible. In this approach, we could look at where data is and where data isnt, and see if we can find 'groups' of data that are missing some data, but not others. We could try doing this in a column-wise, row-wise, or column-by-row wise manner. Basically, we can see if we can find from the presense of missing data, some meta groups where most of the data is in-tact. Whichever approach we take, we would end up on different column/ row combinations of data and would be training multiple models on the back and that can then be reconfigured into an ensemble model.\n",
        "\n",
        "I'm planning on focusing primarily on the data clean-up and filtering to get improvements on performance in this exercise."
      ],
      "metadata": {
        "id": "h8bUTDbVbkKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in the data"
      ],
      "metadata": {
        "id": "H1TxvJ0PKXDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')"
      ],
      "metadata": {
        "id": "1CrZ-7Rwb2Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YxcUVfNCFePX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So lets take a look at our data."
      ],
      "metadata": {
        "id": "K8fn_tsgKjSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().describe()"
      ],
      "metadata": {
        "id": "hoOz7Rlb-g_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So even in our first row we can see what the degree of missing data looks like. The first row is missing information on age, sex, race, ethnicity. Not training on these data would be less than desireable."
      ],
      "metadata": {
        "id": "D5KyBD8OKplA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[:1]"
      ],
      "metadata": {
        "id": "hfzWxbHOEQac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So if we look for null data, we find that most rows are missing at least some data. A minimally complete list would be composed of 'case_month', 'process', 'labconfirmed_yn'. Even our response variable, 'death_yn' is missing data."
      ],
      "metadata": {
        "id": "u3LK9qy-K6ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().describe()"
      ],
      "metadata": {
        "id": "fKWM_sy3A5gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also want to deal with other issues the data may have, specifically, there are several rows that have 'confusing' or confused data. We should also take the time to standarsize our data, but also set any very uncommon values to NA where appropriate. Effectively, we neeed to define a data dictionary to support us through the rest of this investigation."
      ],
      "metadata": {
        "id": "Lx1d6RWzL8NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in train.columns:\n",
        "#   print('Unique items '+i)\n",
        "#   print(train[i].unique())"
      ],
      "metadata": {
        "id": "fn5ft0DzLq4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I'm going to ignore the correctness of County and State. I still want to use these data, but I can't know if spellings are all 100% correct, or at least, I don't think its worthe the time. Age group seems pretty well formed. The 'sex' column mayneed some work, but we can see that race has the same classes for alternative ways data couold be missing. We'll have to make a choice as to how to handle 'true' missing data.\n",
        "\n",
        "Other columns with issues appear to be exposure_yn, lab_confirmed_yn, symptomatic_yn, basically all of the columns. Some of the issues are that they arent stanadardized into either float, string, or booleans. Also, there are some mixes and some text 'nulls' peppered in there. We'll need to address all of these before procceding.\n",
        "\n",
        "Another consideration as part of this process, is that this NA processing and filtering has to happen on both our test and validation sets. We'll need to keep this in mind in how we implement our solution.\n"
      ],
      "metadata": {
        "id": "zojCIu1TPMgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a weird one because we know they were all exposed, so not-knowing not\n",
        "#really a nan\n",
        "import numpy as np\n",
        "\n",
        "train['exposure_yn'][train['exposure_yn'].isnull()] = 0.0\n",
        "train['exposure_yn'][train['exposure_yn']== 1] = 1.0\n",
        "# train['exposure_yn'] = train['exposure_yn'].astype('float')\n",
        "\n",
        "\n",
        "train['labconfirmed_yn'][train['labconfirmed_yn']== 1] = 1.0\n",
        "train['labconfirmed_yn'][train['labconfirmed_yn']== 0] = 0.0\n",
        "# train['labconfirmed_yn'] = train['labconfirmed_yn'].astype('float')\n",
        "\n",
        "train['symptomatic_yn'][train['symptomatic_yn']== '1'] = 1.0\n",
        "train['symptomatic_yn'][train['symptomatic_yn']== '0'] = 0.0\n",
        "# train['symptomatic_yn'] = train['symptomatic_yn'].astype('float')\n",
        "\n",
        "\n",
        "\n",
        "train['hosp_yn'][train['hosp_yn']== 1] = 1.0\n",
        "train['hosp_yn'][train['hosp_yn']== 0] = 0.0\n",
        "# train['hosp_yn'] = train['hosp_yn'].astype('float')\n",
        "\n",
        "train['icu_yn'][train['icu_yn']== '1'] = 1.0\n",
        "train['icu_yn'][train['icu_yn']== '0'] = 0.0\n",
        "train['icu_yn'][train['icu_yn']== 'nul'] = np.nan\n",
        "# train['icu_yn'] = train['icu_yn'].astype('float')\n"
      ],
      "metadata": {
        "id": "gKYxCUtQTfdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train.columns:\n",
        "  print('Unique items '+i)\n",
        "  print(train[i].unique())"
      ],
      "metadata": {
        "id": "xmF-GCaIcJlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, at least now our data is some what standardized and we can look for clustering among our NA. So that we can remember what tha looks like:"
      ],
      "metadata": {
        "id": "75cgXa6nhEcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().describe()"
      ],
      "metadata": {
        "id": "8tIFGPvVhlJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to convert classes to booleans, so we're going to drop some redundant columns"
      ],
      "metadata": {
        "id": "IgCf9jHCicrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "id": "TUGH5KMyjWBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "na_cluster_test = train[['case_month', 'res_state', 'res_county',\n",
        "       'age_group', 'sex', 'race', 'ethnicity',\n",
        "       'case_positive_specimen_interval', 'case_onset_interval', 'process',\n",
        "       'exposure_yn', 'labconfirmed_yn', 'symptomatic_yn', 'hosp_yn', 'icu_yn',\n",
        "       'death_yn', 'underlying_conditions_yn']]\n",
        "\n",
        "na_cluster_test = train.isnull()\n",
        "na_cluster_test = na_cluster_test.astype(int)"
      ],
      "metadata": {
        "id": "P18eJ6nvjRPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(5, random_state=0)\n",
        "kmeans.fit(na_cluster_test)\n",
        "identified_clusters = kmeans.fit_predict(na_cluster_test)\n",
        "identified_clusters"
      ],
      "metadata": {
        "id": "PRABrJMxhv--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what we're looking for here is any strong or weak groupings where its almost all one or the other, effectively, autocorrelation between NA's in some columns."
      ],
      "metadata": {
        "id": "Kouj6O95nemX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "na_cluster_test.groupby(identified_clusters).sum() / na_cluster_test.sum()"
      ],
      "metadata": {
        "id": "8UubgzainY5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whats interesting here, is that most of the clusters are pretty evenly split. However, there appears to be one cluster which sticks out and contains all the NA's from res_state, state_fips codes, age, sex, most of race, and most of ethnicity. Most of the other columns are more spread out between the clusters. It would like make sense then to split our modeling. We can make one model that predicts based on res_state, state_fips codes, age, sex, most of race, and ethnicity, since we can expect most of those columns to be complete where the others are complete. We can then place the rest of the columns in another model. In this way we can minimize the total amount of imputation we have to rely on, should we choose to rely on that. We can also consider not doing imputation and just running the two models where we have data."
      ],
      "metadata": {
        "id": "8iCPRzBWsv2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a method for imputting random features into the dataset"
      ],
      "metadata": {
        "id": "iTQxfljkvkEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def randomiseMissingData(df):\n",
        "    import random\n",
        "    \"randomise missing data for DataFrame (within a column)\"\n",
        "    # df = df2.copy()\n",
        "    for col in df.columns:\n",
        "        data = df[col]\n",
        "        mask = data.isnull()\n",
        "        samples = random.choices( data[~mask].values , k = mask.sum() )\n",
        "        df.loc[mask,col] = samples\n",
        "    return df"
      ],
      "metadata": {
        "id": "tvfFl9A9_2S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "id": "mav8hZxwxV29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_fill = train[identified_clusters != 2]\n",
        "train_fill['group'] = 'train'\n",
        "test['group'] = 'test'\n",
        "test['death_yn'] = 0\n",
        "small_cluster_train = train_fill.append(test)"
      ],
      "metadata": {
        "id": "OjviFCbiike9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# small_cluster_train = train[identified_clusters != 2]\n",
        "small_cluster_train = small_cluster_train[['case_month','res_state','age_group','sex','race','ethnicity','death_yn','group']]\n",
        "small_cluster_train\t = randomiseMissingData(small_cluster_train)\n",
        "small_cluster_train = pd.get_dummies(small_cluster_train)\t \n",
        "small_cluster_train = small_cluster_train[train['death_yn'].notnull()]"
      ],
      "metadata": {
        "id": "NNeU-51tvyOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_cluster_train.columns"
      ],
      "metadata": {
        "id": "XP0dl9LIki5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# small_cluster_train['group_train']\n",
        "train_filled = small_cluster_train[small_cluster_train['group_train'] == 1]\n",
        "test_filled = small_cluster_train[small_cluster_train['group_train'] == 0]"
      ],
      "metadata": {
        "id": "4jhDoikMj8L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train_filled['death_yn']\n",
        "del train_filled['death_yn']\n",
        "del test_filled['death_yn']"
      ],
      "metadata": {
        "id": "LfzXMmSEzCJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y))\n",
        "print(len(small_cluster_train))"
      ],
      "metadata": {
        "id": "gfhpuYzu0o81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "bag = BaggingClassifier(tree, n_estimators=100, max_samples=.8,\n",
        "                        random_state=1)\n",
        "\n",
        "bag.fit(train_filled, y)"
      ],
      "metadata": {
        "id": "NJ_nkHp3FOwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y_pred = bag.predict(test_filled)"
      ],
      "metadata": {
        "id": "dky_sq54H3JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create submission file.\n",
        "submission = pd.DataFrame(test_y_pred, columns=['prediction']) # Create new dataframe.\n",
        "submission['Id'] = submission.index  # Kaggle expects two columns: Id, prediction.\n",
        "submission.to_csv('sample_submission.csv', index=False)\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "plt.hist(test_y_pred, bins=100);"
      ],
      "metadata": {
        "id": "mGvAFpmJo7eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So you can immediately notice some pretty big differences between the two dataset. Firstly, more counties and states in training than test. Probably a true random sample (which could mean that some rare counties or combinations of groups don't exist in the validation set). Same with sex, pretty big difference between the two. However thats a little weirder because there so many. The two ratios should be much closer than that."
      ],
      "metadata": {
        "id": "TfNo6s8Rdbsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission\n",
        "!kaggle competitions submit covid-19-risk-2022 -f sample_submission.csv -m 'Column Coherence'"
      ],
      "metadata": {
        "id": "tt5qq-wwpe8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code for training model and creating submission file.\n",
        "# Author: Peter Sadowski Jan 22 2022\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load training data.\n",
        "df_train = pd.read_csv('./train_small.csv.zip') # Can read from zip files directly.\n",
        "df_train = df_train.replace({'death_yn':{np.nan:0}}) # Assume no info means survived.\n",
        "y = df_train['death_yn']\n",
        "\n",
        "# Load test data.\n",
        "df_test = pd.read_csv('./test.csv.zip')\n",
        "\n",
        "# Encode state variable as one-hot. \n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "enc.fit(df_train[['res_state']])\n",
        "state_train = enc.transform(df_train[['res_state']])\n",
        "state_test = enc.transform(df_test[['res_state']])\n",
        "\n",
        "# Combine this with whether patient went to ICU.\n",
        "X = np.concatenate([df_train[['icu_yn']]==1, state_train], axis=1)\n",
        "X_test = np.concatenate([df_test[['icu_yn']] == 1, state_test], axis=1)\n",
        "\n",
        "# Make predictions based on whether patient went to ICU, and their state.\n",
        "model = LogisticRegression()\n",
        "model.fit(X,y)\n",
        "ypred = model.predict_proba(X_test)[:,1]\n",
        "print(f'Model coefficients: {model.coef_}')\n",
        "\n",
        "# Create submission file.\n",
        "submission = pd.DataFrame(ypred, columns=['prediction']) # Create new dataframe.\n",
        "submission['Id'] = submission.index  # Kaggle expects two columns: Id, prediction.\n",
        "submission.to_csv('sample_submission.csv', index=False)\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "plt.hist(ypred, bins=100);"
      ],
      "metadata": {
        "id": "CBz09mTLWJ1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission\n",
        "!kaggle competitions submit covid-19-risk-2022 -f sample_submission.csv -m 'Heres Johnny'\n"
      ],
      "metadata": {
        "id": "GVoOaCANWUF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SxdB6P_ulkfa"
      },
      "cell_type": "markdown",
      "source": [
        "## Uploading a Colab notebook to Kaggle Kernels\n",
        "\n",
        "Bear with us, as this is a little round-about..."
      ]
    },
    {
      "metadata": {
        "id": "o-NDs2dImDG0"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading a notebook from Colab\n",
        "\n",
        "To download from Colab, use **File** | **Download .ipynb**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# user = \"ics435\"\n",
        "# repo = \"ps1-numpy-seismosmsr\"\n",
        "# src_dir = \"master\"\n",
        "# pyfile = \"kaggle_api_in_colab.ipynb\"\n",
        "# raw_git = 'https://raw.githubusercontent.com/seismosmsr/machine_learning/main/kaggle_api_in_colab.ipynb'\n",
        "\n",
        "# url = f\"{raw_git}\"\n",
        "\n",
        "# !wget --no-cache --backups=1 {url} -o submission.ipynb"
      ],
      "metadata": {
        "id": "qAYSRjCE_IyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zUMAuin2mbbW"
      },
      "cell_type": "markdown",
      "source": [
        "### Then upload the notebook to your Colab runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # uploaded = files.upload()\n",
        "# notebook_path = '/content/kaggle_api_in_colab.ipynb'"
      ],
      "metadata": {
        "id": "tL6TK3xxwqjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qox_MozfmF2Y"
      },
      "cell_type": "code",
      "source": [
        "# uploaded = files.upload()\n",
        "# notebook_path = list(uploaded.keys())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BT_rPcuoMsd"
      },
      "cell_type": "code",
      "source": [
        "# !mkdir -p export\n",
        "# !mv $notebook_path export/\n",
        "# !kaggle kernels init -p export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ls0Umf85q9UE"
      },
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# import random\n",
        "# your_kaggle_username = 'Aron Boettcher'\n",
        "# notebook_title = 'Test Kernel ' + str(random.randint(1,100))\n",
        "# new_kernel_slug = re.sub(r'[^a-z0-9]+', '-', notebook_title.lower())\n",
        "# notebook_path = 'kaggle_api_in_colab.ipynb'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rp62OpkCo5D3"
      },
      "cell_type": "code",
      "source": [
        "# # Documented here: https://github.com/Kaggle/kaggle-api/wiki/Kernel-Metadata\n",
        "# metadata = '''\n",
        "# {\n",
        "#   \"id\": \"%s/%s\",\n",
        "#   \"title\": \"%s\",\n",
        "#   \"code_file\": \"%s\",\n",
        "#   \"language\": \"python\",\n",
        "#   \"kernel_type\": \"notebook\",\n",
        "#   \"is_private\": \"true\",\n",
        "#   \"enable_gpu\": \"false\",\n",
        "#   \"enable_internet\": \"true\",\n",
        "#   \"dataset_sources\": [],\n",
        "#   \"competition_sources\": [],\n",
        "#   \"kernel_sources\": []\n",
        "# }\n",
        "# ''' % (your_kaggle_username, new_kernel_slug, notebook_title, notebook_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PyX-cbzIoj-V"
      },
      "cell_type": "code",
      "source": [
        "# !echo '$metadata' > export/kernel-metadata.json\n",
        "# !cat export/kernel-metadata.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VDP5xJPklqYk"
      },
      "cell_type": "code",
      "source": [
        "# !kaggle kernels push -p export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the functions:\n",
        "def stratified_sample(df, strata, size=None, seed=None, keep_index= True):\n",
        "    '''\n",
        "    It samples data from a pandas dataframe using strata. These functions use\n",
        "    proportionate stratification:\n",
        "    n1 = (N1/N) * n\n",
        "    where:\n",
        "        - n1 is the sample size of stratum 1\n",
        "        - N1 is the population size of stratum 1\n",
        "        - N is the total population size\n",
        "        - n is the sampling size\n",
        "    Parameters\n",
        "    ----------\n",
        "    :df: pandas dataframe from which data will be sampled.\n",
        "    :strata: list containing columns that will be used in the stratified sampling.\n",
        "    :size: sampling size. If not informed, a sampling size will be calculated\n",
        "        using Cochran adjusted sampling formula:\n",
        "        cochran_n = (Z**2 * p * q) /e**2\n",
        "        where:\n",
        "            - Z is the z-value. In this case we use 1.96 representing 95%\n",
        "            - p is the estimated proportion of the population which has an\n",
        "                attribute. In this case we use 0.5\n",
        "            - q is 1-p\n",
        "            - e is the margin of error\n",
        "        This formula is adjusted as follows:\n",
        "        adjusted_cochran = cochran_n / 1+((cochran_n -1)/N)\n",
        "        where:\n",
        "            - cochran_n = result of the previous formula\n",
        "            - N is the population size\n",
        "    :seed: sampling seed\n",
        "    :keep_index: if True, it keeps a column with the original population index indicator\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    A sampled pandas dataframe based in a set of strata.\n",
        "    Examples\n",
        "    --------\n",
        "    >> df.head()\n",
        "    \tid  sex age city \n",
        "    0\t123 M   20  XYZ\n",
        "    1\t456 M   25  XYZ\n",
        "    2\t789 M   21  YZX\n",
        "    3\t987 F   40  ZXY\n",
        "    4\t654 M   45  ZXY\n",
        "    ...\n",
        "    # This returns a sample stratified by sex and city containing 30% of the size of\n",
        "    # the original data\n",
        "    >> stratified = stratified_sample(df=df, strata=['sex', 'city'], size=0.3)\n",
        "    Requirements\n",
        "    ------------\n",
        "    - pandas\n",
        "    - numpy\n",
        "    '''\n",
        "    population = len(df)\n",
        "    size = __smpl_size(population, size)\n",
        "    tmp = df[strata]\n",
        "    tmp['size'] = 1\n",
        "    tmp_grpd = tmp.groupby(strata).count().reset_index()\n",
        "    tmp_grpd['samp_size'] = round(size/population * tmp_grpd['size']).astype(int)\n",
        "\n",
        "    # controlling variable to create the dataframe or append to it\n",
        "    first = True \n",
        "    for i in range(len(tmp_grpd)):\n",
        "        # query generator for each iteration\n",
        "        qry=''\n",
        "        for s in range(len(strata)):\n",
        "            stratum = strata[s]\n",
        "            value = tmp_grpd.iloc[i][stratum]\n",
        "            n = tmp_grpd.iloc[i]['samp_size']\n",
        "\n",
        "            if type(value) == str:\n",
        "                value = \"'\" + str(value) + \"'\"\n",
        "            \n",
        "            if s != len(strata)-1:\n",
        "                qry = qry + stratum + ' == ' + str(value) +' & '\n",
        "            else:\n",
        "                qry = qry + stratum + ' == ' + str(value)\n",
        "        \n",
        "        # final dataframe\n",
        "        if first:\n",
        "            stratified_df = df.query(qry).sample(n=n, random_state=seed).reset_index(drop=(not keep_index))\n",
        "            first = False\n",
        "        else:\n",
        "            tmp_df = df.query(qry).sample(n=n, random_state=seed).reset_index(drop=(not keep_index))\n",
        "            stratified_df = stratified_df.append(tmp_df, ignore_index=True)\n",
        "    \n",
        "    return stratified_df\n",
        "\n",
        "\n",
        "\n",
        "def stratified_sample_report(df, strata, size=None):\n",
        "    '''\n",
        "    Generates a dataframe reporting the counts in each stratum and the counts\n",
        "    for the final sampled dataframe.\n",
        "    Parameters\n",
        "    ----------\n",
        "    :df: pandas dataframe from which data will be sampled.\n",
        "    :strata: list containing columns that will be used in the stratified sampling.\n",
        "    :size: sampling size. If not informed, a sampling size will be calculated\n",
        "        using Cochran adjusted sampling formula:\n",
        "        cochran_n = (Z**2 * p * q) /e**2\n",
        "        where:\n",
        "            - Z is the z-value. In this case we use 1.96 representing 95%\n",
        "            - p is the estimated proportion of the population which has an\n",
        "                attribute. In this case we use 0.5\n",
        "            - q is 1-p\n",
        "            - e is the margin of error\n",
        "        This formula is adjusted as follows:\n",
        "        adjusted_cochran = cochran_n / 1+((cochran_n -1)/N)\n",
        "        where:\n",
        "            - cochran_n = result of the previous formula\n",
        "            - N is the population size\n",
        "    Returns\n",
        "    -------\n",
        "    A dataframe reporting the counts in each stratum and the counts\n",
        "    for the final sampled dataframe.\n",
        "    '''\n",
        "    population = len(df)\n",
        "    size = __smpl_size(population, size)\n",
        "    tmp = df[strata]\n",
        "    tmp['size'] = 1\n",
        "    tmp_grpd = tmp.groupby(strata).count().reset_index()\n",
        "    tmp_grpd['samp_size'] = round(size/population * tmp_grpd['size']).astype(int)\n",
        "    return tmp_grpd\n",
        "\n",
        "\n",
        "def __smpl_size(population, size):\n",
        "    '''\n",
        "    A function to compute the sample size. If not informed, a sampling \n",
        "    size will be calculated using Cochran adjusted sampling formula:\n",
        "        cochran_n = (Z**2 * p * q) /e**2\n",
        "        where:\n",
        "            - Z is the z-value. In this case we use 1.96 representing 95%\n",
        "            - p is the estimated proportion of the population which has an\n",
        "                attribute. In this case we use 0.5\n",
        "            - q is 1-p\n",
        "            - e is the margin of error\n",
        "        This formula is adjusted as follows:\n",
        "        adjusted_cochran = cochran_n / 1+((cochran_n -1)/N)\n",
        "        where:\n",
        "            - cochran_n = result of the previous formula\n",
        "            - N is the population size\n",
        "    Parameters\n",
        "    ----------\n",
        "        :population: population size\n",
        "        :size: sample size (default = None)\n",
        "    Returns\n",
        "    -------\n",
        "    Calculated sample size to be used in the functions:\n",
        "        - stratified_sample\n",
        "        - stratified_sample_report\n",
        "    '''\n",
        "    if size is None:\n",
        "        cochran_n = round(((1.96)**2 * 0.5 * 0.5)/ 0.02**2)\n",
        "        n = round(cochran_n/(1+((cochran_n -1) /population)))\n",
        "    elif size >= 0 and size < 1:\n",
        "        n = round(population * size)\n",
        "    elif size < 0:\n",
        "        raise ValueError('Parameter \"size\" must be an integer or a proportion between 0 and 0.99.')\n",
        "    elif size >= 1:\n",
        "        n = size\n",
        "    return n\n"
      ],
      "metadata": {
        "id": "6ld6m5ouSA7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}