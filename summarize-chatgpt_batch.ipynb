{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA0rJ3mVd3zhOD0upLGvBP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seismosmsr/machine_learning/blob/main/summarize-chatgpt_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for google colab you have to install this 2 library before run the code\n",
        "!pip install pypdf2\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve9h8kzcAdRn",
        "outputId": "04e81a2a-6771-45b9-cdee-b372540ee1e4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import PyPDF2\n",
        "import json\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import zipfile\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LnvLZeaq-5WG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_output_files(output_directory):\n",
        "    files = os.listdir(output_directory)\n",
        "    for file_path in files:\n",
        "        try:\n",
        "            os.remove(os.path.join(output_directory, file_path))\n",
        "            print(f\"Successfully deleted {file_path}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "hFrDpHAGACl4"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_texts(texts, chunk_size=1000, max_chunks=10, overlap_size=200):\n",
        "    \"\"\"\n",
        "    Merge a list of texts into larger chunks of text and track the origin of each chunk.\n",
        "\n",
        "    :param texts: List of texts to be merged\n",
        "    :param chunk_size: Preferred size of each merged chunk\n",
        "    :param max_chunks: Maximum number of chunks to be created\n",
        "    :param overlap_size: Number of characters to overlap between chunks\n",
        "    :return: Tuple of two lists: merged chunks and list of unique paragraph indices for each chunk\n",
        "    \"\"\"\n",
        "    if not texts:\n",
        "        return [], []\n",
        "\n",
        "    total_length = sum(len(text) for text in texts)\n",
        "    if total_length > max_chunks * (chunk_size - overlap_size):\n",
        "        chunk_size = (total_length + overlap_size * (max_chunks - 1)) // max_chunks\n",
        "\n",
        "    merged_chunks = []\n",
        "    chunk_indices = []\n",
        "    current_chunk = \"\"\n",
        "    current_indices = set()\n",
        "    remaining_overlap = \"\"\n",
        "    remaining_indices = set()\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if len(current_chunk) + len(word) + 1 > chunk_size:\n",
        "                merged_chunks.append(current_chunk.strip())\n",
        "                chunk_indices.append(sorted(current_indices))\n",
        "                current_chunk = remaining_overlap + \" \"\n",
        "                current_indices = remaining_indices.copy()\n",
        "                remaining_overlap = \"\"\n",
        "                remaining_indices = set()\n",
        "                if len(merged_chunks) == max_chunks - 1:\n",
        "                    remaining_texts = \" \".join(texts[i:])\n",
        "                    merged_chunks.append(current_chunk + remaining_texts)\n",
        "                    remaining_indices.add(i)\n",
        "                    chunk_indices.append(sorted(current_indices.union(remaining_indices)))\n",
        "                    return merged_chunks, chunk_indices\n",
        "            current_chunk += word + \" \"\n",
        "            if len(current_chunk) > chunk_size - overlap_size:\n",
        "                remaining_overlap += word + \" \"\n",
        "                remaining_indices.add(i)\n",
        "            current_indices.add(i)\n",
        "\n",
        "    if current_chunk:\n",
        "        merged_chunks.append(current_chunk.strip())\n",
        "        chunk_indices.append(sorted(current_indices))\n",
        "\n",
        "    return merged_chunks, chunk_indices\n"
      ],
      "metadata": {
        "id": "OgmUOE7bvjhe"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import openai\n",
        "\n",
        "def read_pdf_and_summarize(file_title, run_id, output_directory, max_retries=3, delay=300):\n",
        "    pdf_summary_text = \"\"\n",
        "    pdf_file_path = os.path.join('/content/crop_paper_share/', file_title)\n",
        "\n",
        "    with open(pdf_file_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page_text = pdf_reader.pages[page_num].extract_text().lower()\n",
        "            paragraphs = [line.strip() for line in page_text.split('\\n \\n') if line.strip()]\n",
        "            chunks, paragraphs = merge_texts(paragraphs, chunk_size=3000, overlap_size=1000)\n",
        "            # chunks, paragraphs\n",
        "            for para_text, para_num in zip(chunks, paragraphs):\n",
        "                retries = 0\n",
        "                para_num = ', '.join(map(str, para_num))\n",
        "                while retries < max_retries:\n",
        "                    try:\n",
        "                        response = openai.ChatCompletion.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"You are a helpful research assistant. Specfically, you are helping research the impact of climate change on global food systems. We are doing a literature review. We want to know what crops and food systems are being impacted by specific climate change hazards. Only respond in the form of comma seperated values (csv). You always return a correctly formatted csv.\"},\n",
        "                                {\"role\": \"user\", \"content\": f\"Summarize every sentence of the following text as a JSON document. Do not reply with anything except a JSON document. Please identify all food systems (crop, animal, or wild), any potential climate change hazards to food systems, what cropping or food systems (such as a fishery) they effect, where globally the impact will be experienced, the specific quote from the paragraph (of at least 100 characters), if the impact is generally positive or negative (sentiment), and approximately what magnitude (e.g. high medium low). Do not do anything that could possibly break JSON formatting. Please make sure that text entries do not use commas internal to any text entries in the table. Please only return a JSON. The elements should only be: region, cropping_system, impact, sentiment, magnitude, quote, page_number (you are working on page {page_num}), paragraph_number (you are working on paragraph numbers: {para_num}). Here is the text:{page_text}. Do not return anything but the properly formatted JSON. It is of the utmost importance that the response you give is a properly formatted JSON. If the paragraph does not contain any text about climate change hazards, return an empty JSON (the categories with the text no data as the content) \"},\n",
        "                                ])\n",
        "\n",
        "                        page_summary = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "                        pdf_summary_text += page_summary + \"\\n\"\n",
        "\n",
        "                        page_summary_file = os.path.splitext(file_title)[0] + f\"_{page_num}_{para_num}_summary.json\"\n",
        "                        with open(os.path.join(output_directory, page_summary_file), \"w+\") as file:\n",
        "                            file.write(page_summary)\n",
        "\n",
        "                        break  # Break out of the retry loop since the request was successful\n",
        "\n",
        "                    except openai.error.OpenAIError as e:\n",
        "                        if \"timeout\" in str(e).lower():\n",
        "                            print(f\"Request timed out. Retrying in {delay} seconds...\")\n",
        "                            retries += 1\n",
        "                            time.sleep(delay)\n",
        "                        else:\n",
        "                            print(\"An error occurred:\", str(e))\n",
        "                            break  # Break out of the retry loop for non-timeout errors\n",
        "                else:\n",
        "                    print(\"Max retries reached. Moving to the next paragraph.\")\n",
        "\n",
        "    pdf_summary_file = os.path.splitext(file_title)[0] + \"_summary.txt\"\n",
        "    with open(os.path.join(output_directory, pdf_summary_file), \"w+\") as file:\n",
        "        file.write(pdf_summary_text)\n"
      ],
      "metadata": {
        "id": "_j07ezZvAFV_"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_summaries(output_directory, file_title, run_id):\n",
        "    files = [f for f in os.listdir(output_directory) if f.endswith('.json')]\n",
        "    dfs = []\n",
        "    failed_files = []\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            with open(os.path.join(output_directory, file), 'r') as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
        "                    df = pd.DataFrame(data)\n",
        "                    dfs.append(df)\n",
        "                elif isinstance(data, dict):\n",
        "                    df = pd.DataFrame([data])\n",
        "                    dfs.append(df)\n",
        "                else:\n",
        "                    print(f\"Unhandled data structure in {file}. Skipping.\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Failed to decode JSON for {file}. Adding to failed list.\")\n",
        "            failed_files.append(file)\n",
        "\n",
        "    if dfs:\n",
        "        combined_df = pd.concat(dfs, ignore_index=True)\n",
        "        combined_df['paper'] = file_title\n",
        "        combined_df['run'] = run_id\n",
        "        combined_df.to_csv('merged_output.csv', index=False)\n",
        "\n",
        "        if failed_files:\n",
        "            failed_df = pd.DataFrame(failed_files, columns=[\"Failed Filenames\"])\n",
        "            failed_df.to_csv('failed_files.csv', index=False)\n",
        "\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No data to process.\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "vmy_PyguAITw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_to_google_sheets(combined_df):\n",
        "    # Convert non-serializable types (like np.int64 and np.float64) to native Python types\n",
        "    converted_df = combined_df.applymap(lambda x: int(x) if isinstance(x, np.integer) else (float(x) if isinstance(x, np.floating) else x))\n",
        "\n",
        "    # Convert all values to string to ensure compatibility\n",
        "    str_df = converted_df.astype(str)\n",
        "\n",
        "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name('/content/precise-duality-203214-f20314634651.json', scope)\n",
        "    client = gspread.authorize(creds)\n",
        "    sheet = client.open_by_key(\"1iM2fqvMhSsf11uLoWTfAU7cpKhSM0NnLx6MB7bmG9fM\").sheet1\n",
        "\n",
        "    data_to_append = str_df.values.tolist()\n",
        "    sheet.append_rows(data_to_append)"
      ],
      "metadata": {
        "id": "ezPta56uAKkN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "file_id = \"1jP3C9kxyFhYFLff1_aCW2IoOD9-y-hTc\"\n",
        "destination = \"publications.zip\"\n",
        "\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(destination, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"Download complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zd_FMEEARBe",
        "outputId": "22f9c832-99c0-4473-b239-f31b15df98cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the contents of the ZIP file\n",
        "with zipfile.ZipFile('/content/publications.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "metadata": {
        "id": "Q-E_o7BWAXJv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_list = [f for f in os.listdir('/content/crop_paper_share') if f.endswith('.pdf')]\n",
        "output_directory = '/content/output/'"
      ],
      "metadata": {
        "id": "7g77EqpPArQZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.mkdir('/content/output/')"
      ],
      "metadata": {
        "id": "uglpQHoGA9_H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir('/content/output/')\n",
        "for file_path in files:\n",
        "    try:\n",
        "        os.remove('/content/output/'+file_path)\n",
        "        print(f\"Successfully deleted {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "ws-s-mIUA5_h"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# openai.api_key = \"NA\" #change the api key with yours"
      ],
      "metadata": {
        "id": "yfnq3VBYBaZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not combined_df.empty:\n",
        "    upload_to_google_sheets(combined_df)"
      ],
      "metadata": {
        "id": "YZ9xZ0BwZGkR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not combined_df.empty:\n",
        "    upload_to_google_sheets(combined_df)"
      ],
      "metadata": {
        "id": "-xgEMtr3wlPe"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for run_id in range(10):\n",
        "  for file_title in [files_list[8]]:\n",
        "    delete_output_files(output_directory)\n",
        "    read_pdf_and_summarize(file_title, run_id, output_directory)\n",
        "    combined_df = process_summaries(output_directory, file_title, run_id)\n",
        "    if not combined_df.empty:\n",
        "        upload_to_google_sheets(combined_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5wjtn2O-83a",
        "outputId": "15e73ead-9469-4039-b634-dc0dd4cf4614"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json. Adding to failed list.\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json. Adding to failed list.\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n",
            "An error occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 27 Oct 2023 07:34:50 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '81c9339e88432c15-SLC', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
            "An error occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 27 Oct 2023 07:45:32 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '81c9434beb165308-SLC', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json. Adding to failed list.\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json. Adding to failed list.\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n",
            "Request timed out. Retrying in 300 seconds...\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json. Adding to failed list.\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json. Adding to failed list.\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json. Adding to failed list.\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n",
            "Request timed out. Retrying in 300 seconds...\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json. Adding to failed list.\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n",
            "An error occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 27 Oct 2023 08:31:50 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '81c9871f3c1627d8-SLC', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json. Adding to failed list.\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json. Adding to failed list.\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json. Adding to failed list.\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json. Adding to failed list.\n",
            "Failed to decode JSON for LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json. Adding to failed list.\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_6_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_summary.txt\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_4_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_0_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_1_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_5_0_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_2_0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37_summary.json\n",
            "Successfully deleted LaurusNobilis_SCI_Q4_researchgate_3_0_summary.json\n"
          ]
        }
      ]
    }
  ]
}