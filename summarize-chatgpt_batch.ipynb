{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8dSDqVraXNiU1S6FpTq01",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seismosmsr/machine_learning/blob/main/summarize-chatgpt_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for google colab you have to install this 2 library before run the code\n",
        "!pip install pypdf2\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve9h8kzcAdRn",
        "outputId": "91402107-c621-4786-c9b5-b4b6ec9cc902"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/232.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf2\n",
            "Successfully installed pypdf2-3.0.1\n",
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import PyPDF2\n",
        "import json\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "LnvLZeaq-5WG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_output_files(output_directory):\n",
        "    files = os.listdir(output_directory)\n",
        "    for file_path in files:\n",
        "        try:\n",
        "            os.remove(os.path.join(output_directory, file_path))\n",
        "            print(f\"Successfully deleted {file_path}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "hFrDpHAGACl4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf_and_summarize(file_title, run_id, output_directory):\n",
        "    pdf_summary_text = \"\"\n",
        "    pdf_file_path = os.path.join('/content/crop_paper_share/', file_title)\n",
        "\n",
        "    with open(pdf_file_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page_text = pdf_reader.pages[page_num].extract_text().lower()\n",
        "            paragraphs = [line.strip() for line in page_text.split('\\n \\n') if line.strip()]\n",
        "            for para_num, para_text in enumerate(paragraphs):\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful research assistant. Specifically, you are helping research the impact of climate change on global food systems. We are doing a literature review. We want to know what crops and food systems are being impacted by specific climate change hazards. Only respond in the form of comma separated values (csv). You always return a correctly formatted csv.\"},\n",
        "                        {\"role\": \"user\", \"content\": f\"Summarize the following text as a JSON document. ... (your instructions here)... Here is the text: {para_text}\"}\n",
        "                    ])\n",
        "\n",
        "                page_summary = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "                pdf_summary_text += page_summary + \"\\n\"\n",
        "                page_summary_file = os.path.splitext(file_title)[0] + f\"_{page_num}_{para_num}_summary.json\"\n",
        "                with open(os.path.join(output_directory, page_summary_file), \"w+\") as file:\n",
        "                    file.write(page_summary)\n",
        "\n",
        "    pdf_summary_file = os.path.splitext(file_title)[0] + \"_summary.txt\"\n",
        "    with open(os.path.join(output_directory, pdf_summary_file), \"w+\") as file:\n",
        "        file.write(pdf_summary_text)"
      ],
      "metadata": {
        "id": "_j07ezZvAFV_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_summaries(output_directory, file_title, run_id):\n",
        "    files = [f for f in os.listdir(output_directory) if f.endswith('.json')]\n",
        "    dfs = []\n",
        "    failed_files = []\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            with open(os.path.join(output_directory, file), 'r') as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
        "                    df = pd.DataFrame(data)\n",
        "                    dfs.append(df)\n",
        "                elif isinstance(data, dict):\n",
        "                    df = pd.DataFrame([data])\n",
        "                    dfs.append(df)\n",
        "                else:\n",
        "                    print(f\"Unhandled data structure in {file}. Skipping.\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Failed to decode JSON for {file}. Adding to failed list.\")\n",
        "            failed_files.append(file)\n",
        "\n",
        "    if dfs:\n",
        "        combined_df = pd.concat(dfs, ignore_index=True)\n",
        "        combined_df['paper'] = file_title\n",
        "        combined_df['run'] = run_id\n",
        "        combined_df.to_csv('merged_output.csv', index=False)\n",
        "\n",
        "        if failed_files:\n",
        "            failed_df = pd.DataFrame(failed_files, columns=[\"Failed Filenames\"])\n",
        "            failed_df.to_csv('failed_files.csv', index=False)\n",
        "\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No data to process.\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "vmy_PyguAITw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_to_google_sheets(combined_df):\n",
        "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name('/content/precise-duality-203214-f20314634651.json', scope)\n",
        "    client = gspread.authorize(creds)\n",
        "    sheet = client.open_by_key(\"1iM2fqvMhSsf11uLoWTfAU7cpKhSM0NnLx6MB7bmG9fM\").sheet1\n",
        "\n",
        "    combined_df.fillna('', inplace=True)\n",
        "    data_to_append = [[i for i in combined_df.iloc[j]] for j in range(combined_df.shape[0])]\n",
        "    sheet.append_rows(data_to_append)"
      ],
      "metadata": {
        "id": "ezPta56uAKkN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "file_id = \"1jP3C9kxyFhYFLff1_aCW2IoOD9-y-hTc\"\n",
        "destination = \"publications.zip\"\n",
        "\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(destination, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"Download complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zd_FMEEARBe",
        "outputId": "22f9c832-99c0-4473-b239-f31b15df98cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the contents of the ZIP file\n",
        "with zipfile.ZipFile('/content/publications.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "metadata": {
        "id": "Q-E_o7BWAXJv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_list = [f for f in os.listdir('/content/crop_paper_share') if f.endswith('.pdf')]\n",
        "output_directory = '/content/output/'"
      ],
      "metadata": {
        "id": "7g77EqpPArQZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.mkdir('/content/output/')"
      ],
      "metadata": {
        "id": "uglpQHoGA9_H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir('/content/output/')\n",
        "for file_path in files:\n",
        "    try:\n",
        "        os.remove('/content/output/'+file_path)\n",
        "        print(f\"Successfully deleted {file_path}\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error deleting {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "ws-s-mIUA5_h"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"NA\" #change the api key with yours"
      ],
      "metadata": {
        "id": "yfnq3VBYBaZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for file_title in files_list:\n",
        "    for run_id in range(1):\n",
        "        delete_output_files(output_directory)\n",
        "        read_pdf_and_summarize(file_title, run_id, output_directory)\n",
        "        combined_df = process_summaries(output_directory, file_title, run_id)\n",
        "        if not combined_df.empty:\n",
        "            upload_to_google_sheets(combined_df)"
      ],
      "metadata": {
        "id": "v5wjtn2O-83a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}